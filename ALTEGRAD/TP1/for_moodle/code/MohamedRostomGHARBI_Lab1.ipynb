{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Excraction - Mohamed Rostom GHARBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import itertools\n",
    "import operator\n",
    "import copy\n",
    "import igraph\n",
    "import heapq\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_simple(text, my_stopwords, punct, remove_stopwords=True, pos_filtering=True, stemming=True):\n",
    "    text = text.lower()\n",
    "    text = ''.join(l for l in text if l not in punct) # remove punctuation (preserving intra-word dashes)\n",
    "    text = re.sub(' +',' ',text) # strip extra white space\n",
    "    text = text.strip() # strip leading and trailing white space\n",
    "    # tokenize (split based on whitespace)\n",
    "    # store results as 'tokens' #\n",
    "    tokens = text.split()\n",
    "    ### Split on the space\n",
    "    \n",
    "    if pos_filtering == True:\n",
    "        # POS tag and retain only nouns and adjectives\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        tokens_keep = []\n",
    "        for item in tagged_tokens:\n",
    "            if (\n",
    "            item[1] == 'NN' or\n",
    "            item[1] == 'NNS' or\n",
    "            item[1] == 'NNP' or\n",
    "            item[1] == 'NNPS' or\n",
    "            item[1] == 'JJ' or\n",
    "            item[1] == 'JJS' or\n",
    "            item[1] == 'JJR'\n",
    "            ):\n",
    "                tokens_keep.append(item[0])\n",
    "        tokens = tokens_keep\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # remove stopwords from 'tokens'\n",
    "        tokens = [token for token in tokens if token not in set(my_stopwords)]\n",
    "    \n",
    "    if stemming:\n",
    "        # apply Porter's stemmer\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        tokens_stemmed = list()\n",
    "        for token in tokens:\n",
    "            tokens_stemmed.append(stemmer.stem(token))\n",
    "        tokens = tokens_stemmed\n",
    "    \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_to_graph(terms, w):\n",
    "    '''This function returns a directed, weighted igraph from a list of terms (the tokens from the pre-processed text) e.g., ['quick','brown','fox'].\n",
    "    Edges are weighted based on term co-occurence within a sliding window of fixed size 'w'.\n",
    "    '''\n",
    "    \n",
    "    from_to = {}\n",
    "    \n",
    "    # create initial complete graph (first w terms)\n",
    "    terms_temp = terms[0:w]\n",
    "    indexes = list(itertools.combinations(range(w), r=2))\n",
    "    \n",
    "    new_edges = []\n",
    "    \n",
    "    for my_tuple in indexes:\n",
    "        new_edges.append(tuple([terms_temp[i] for i in my_tuple]))\n",
    "    \n",
    "    for new_edge in new_edges:\n",
    "        if new_edge in from_to:\n",
    "            from_to[new_edge] += 1\n",
    "        else:\n",
    "            from_to[new_edge] = 1\n",
    "\n",
    "    # then iterate over the remaining terms\n",
    "    for i in range(w, len(terms)):\n",
    "        considered_term = terms[i] # term to consider\n",
    "        terms_temp = terms[(i-w+1):(i+1)] # all terms within sliding window\n",
    "        \n",
    "        # edges to try\n",
    "        candidate_edges = []\n",
    "        for p in range(w-1):\n",
    "            candidate_edges.append((terms_temp[p],considered_term))\n",
    "    \n",
    "        for try_edge in candidate_edges:\n",
    "            \n",
    "            if try_edge[1] != try_edge[0]:\n",
    "            # if not self-edge\n",
    "            \n",
    "                # if edge has already been seen, update its weight\n",
    "                if try_edge in from_to: \n",
    "                    from_to[try_edge]+=1\n",
    "                                   \n",
    "                # if edge has never been seen, create it and assign it a unit weight     \n",
    "                else:\n",
    "                    from_to[try_edge]=1 \n",
    "    \n",
    "    # create empty graph\n",
    "    g = igraph.Graph(directed=True)\n",
    "    \n",
    "    # add vertices\n",
    "    g.add_vertices(sorted(set(terms)))\n",
    "    \n",
    "    # add edges, direction is preserved since the graph is directed\n",
    "    g.add_edges(from_to.keys())\n",
    "    \n",
    "    # set edge and vertex weights\n",
    "    g.es['weight'] = list(from_to.values()) # based on co-occurence within sliding window\n",
    "    g.vs['weight'] = g.strength(weights=list(from_to.values())) # weighted degree\n",
    "    \n",
    "    return(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unweighted_k_core(g):\n",
    "    # work on clone of g to preserve g \n",
    "    gg = copy.deepcopy(g)    \n",
    "    \n",
    "    # initialize dictionary that will contain the core numbers\n",
    "    cores_g = dict(zip(gg.vs['name'],[0]*len(gg.vs)))\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # while there are vertices remaining in the graph\n",
    "    while len(gg.vs)>0:\n",
    "        while [deg for deg in gg.strength() if deg<=i]:\n",
    "            index = [ind for ind, deg in enumerate(gg.strength()) if deg<=i][0]\n",
    "            cores_g[gg.vs[index]['name']] = i\n",
    "            gg.delete_vertices(index)\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return cores_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_core_dec(g):\n",
    "    '''\n",
    "    k-core decomposition for weighted graphs (generalized k-cores)\n",
    "    based on Batagelj and Zaversnik's (2002) algorithm #4\n",
    "    '''\n",
    "    # work on clone of g to preserve g \n",
    "    gg = copy.deepcopy(g)    \n",
    "    # initialize dictionary that will contain the core numbers\n",
    "    cores_g = dict(zip(gg.vs[\"name\"],[0]*len(gg.vs[\"name\"])))\n",
    "\n",
    "    # initialize min heap of degrees\n",
    "    heap_g = zip(gg.vs[\"weight\"],gg.vs[\"name\"])\n",
    "    heapq.heapify(list(heap_g))\n",
    "\n",
    "    while len(list(heap_g)) > 0:\n",
    "        \n",
    "        top = heap_g[0][1]\n",
    "        # find vertex index of heap top element\n",
    "        index_top = gg.vs[\"name\"].index(top)\n",
    "        # save names of its neighbors\n",
    "        neighbors_top = gg.vs[gg.neighbors(top)][\"name\"]\n",
    "        # exclude self-edges\n",
    "        neighbors_top = [elt for elt in neighbors_top if elt!=top]\n",
    "        # set core number of heap top element as its weighted degree\n",
    "        cores_g[top] = gg.vs[index_top][\"weight\"]\n",
    "        # delete top vertex\n",
    "        gg.delete_vertices(index_top)\n",
    "        \n",
    "        if len(neighbors_top)>0:\n",
    "        # iterate over neighbors of top element\n",
    "            for i, name_n in enumerate(neighbors_top):\n",
    "                index_n = gg.vs[\"name\"].index(name_n)\n",
    "                max_n = max(cores_g[top],gg.strength(weights=gg.es[\"weight\"])[index_n])\n",
    "                gg.vs[index_n][\"weight\"] = max_n\n",
    "                # update heap\n",
    "                heap_g = zip(gg.vs[\"weight\"],gg.vs[\"name\"])\n",
    "                heapq.heapify(heap_g)\n",
    "        else:\n",
    "            # update heap\n",
    "            heap_g = zip(gg.vs[\"weight\"],gg.vs[\"name\"])\n",
    "            heapq.heapify(heap_g)\n",
    "            \n",
    "    # sort vertices by decreasing core number\n",
    "    sorted_cores_g = dict(sorted(cores_g.items(), key=operator.itemgetter(1), reverse=True))\n",
    "    \n",
    "    return(sorted_cores_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metrics(candidate, truth):\n",
    "    \n",
    "    # true positives ('hits') are both in candidate and in truth\n",
    "    tp = len(set(candidate).intersection(truth))\n",
    "    \n",
    "    # false positives ('false alarms') are in candidate but not in truth\n",
    "    fp = len([element for element in candidate if element not in truth])\n",
    "    \n",
    "    # false negatives ('misses') are in truth but not in candidate\n",
    "    fn = len([element for element in truth if element not in candidate])\n",
    "    \n",
    "    # precision\n",
    "    prec = round(float(tp)/(tp+fp),5)\n",
    "    \n",
    "    # recall\n",
    "    rec = round(float(tp)/(tp+fn),5)\n",
    "    \n",
    "    if prec+rec != 0:\n",
    "        # F1 score\n",
    "        f1 = round(2 * float(prec*rec)/(prec+rec),5)\n",
    "    else:\n",
    "        f1 = 0\n",
    "       \n",
    "    return (prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/rostom/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/rostom/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "0 files processed\n",
      "50 files processed\n",
      "100 files processed\n",
      "150 files processed\n",
      "200 files processed\n",
      "250 files processed\n",
      "300 files processed\n",
      "350 files processed\n",
      "400 files processed\n",
      "450 files processed\n",
      "0 abstracts processed\n",
      "50 abstracts processed\n",
      "100 abstracts processed\n",
      "150 abstracts processed\n",
      "200 abstracts processed\n",
      "250 abstracts processed\n",
      "300 abstracts processed\n",
      "350 abstracts processed\n",
      "400 abstracts processed\n",
      "450 abstracts processed\n",
      "0 files processed\n",
      "50 files processed\n",
      "100 files processed\n",
      "150 files processed\n",
      "200 files processed\n",
      "250 files processed\n",
      "300 files processed\n",
      "350 files processed\n",
      "400 files processed\n",
      "450 files processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import re \n",
    "import itertools\n",
    "import operator\n",
    "import copy\n",
    "import igraph\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# requires nltk >= 3.2.1\n",
    "from nltk import pos_tag\n",
    "\n",
    "# might also be required:\n",
    "#They are required :\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# import custom functions\n",
    "from library import clean_text_simple, terms_to_graph, accuracy_metrics, weighted_core_dec\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stpwds = stopwords.words('english')\n",
    "punct = string.punctuation.replace('-', '')\n",
    "\n",
    "##################################\n",
    "# read and pre-process abstracts #\n",
    "##################################\n",
    "\n",
    "path_to_abstracts = \"../data/Hulth2003testing/abstracts/\"\n",
    "abstract_names = sorted(os.listdir(path_to_abstracts))\n",
    "\n",
    "abstracts = []\n",
    "\n",
    "for counter,filename in enumerate(abstract_names):\n",
    "    # read file\n",
    "    with open(path_to_abstracts + filename, 'r') as my_file: \n",
    "        text = my_file.read().splitlines()\n",
    "    text = ' '.join(text)\n",
    "    # remove formatting\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    abstracts.append(text)\n",
    "    # print progress\n",
    "    if counter % round(len(abstract_names)/10) == 0:\n",
    "        print (counter, 'files processed')\n",
    "\n",
    "abstracts_cleaned = []\n",
    "for counter,abstract in enumerate(abstracts):\n",
    "    my_tokens = clean_text_simple(abstract,my_stopwords=stpwds,punct=punct)\n",
    "    abstracts_cleaned.append(my_tokens)\n",
    "    # print progress\n",
    "    if counter % round(len(abstracts)/10) == 0:\n",
    "        print(counter, 'abstracts processed')\n",
    "                       \n",
    "###############################################\n",
    "# read and pre-process gold standard keywords #\n",
    "###############################################\n",
    "\n",
    "path_to_keywords = \"../data/Hulth2003testing/uncontr/\"\n",
    "keyword_names = sorted(os.listdir(path_to_keywords))\n",
    "   \n",
    "keywords_gold_standard = []\n",
    "\n",
    "for counter,filename in enumerate(keyword_names):\n",
    "    # read file\n",
    "    with open(path_to_keywords + filename, 'r') as my_file: \n",
    "        text = my_file.read().splitlines()\n",
    "    text = ' '.join(text)\n",
    "    text =  re.sub('\\s+', ' ', text) # remove formatting\n",
    "    text = text.lower()\n",
    "    # turn string into list of keywords, preserving intra-word dashes \n",
    "    # but breaking n-grams into unigrams\n",
    "    keywords = text.split(';')\n",
    "    keywords = [keyword.strip().split(' ') for keyword in keywords]\n",
    "    keywords = [keyword for sublist in keywords for keyword in sublist] # flatten list\n",
    "    keywords = [keyword for keyword in keywords if keyword not in stpwds] # remove stopwords (rare but can happen due to n-gram breaking)\n",
    "    keywords_stemmed = [stemmer.stem(keyword) for keyword in keywords]\n",
    "    keywords_stemmed_unique = list(set(keywords_stemmed)) # remove duplicates (can happen due to n-gram breaking)\n",
    "    keywords_gold_standard.append(keywords_stemmed_unique)\n",
    "    \n",
    "    # print progress\n",
    "    if counter % round(len(keyword_names)/10) == 0:\n",
    "        print(counter, 'files processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 abstracts processed\n",
      "50 abstracts processed\n",
      "100 abstracts processed\n",
      "150 abstracts processed\n",
      "200 abstracts processed\n",
      "250 abstracts processed\n",
      "300 abstracts processed\n",
      "350 abstracts processed\n",
      "400 abstracts processed\n",
      "450 abstracts processed\n",
      "0 abstracts processed\n",
      "50 abstracts processed\n",
      "100 abstracts processed\n",
      "150 abstracts processed\n",
      "200 abstracts processed\n",
      "250 abstracts processed\n",
      "300 abstracts processed\n",
      "350 abstracts processed\n",
      "400 abstracts processed\n",
      "450 abstracts processed\n",
      "0 vectors processed\n",
      "50 vectors processed\n",
      "100 vectors processed\n",
      "150 vectors processed\n",
      "200 vectors processed\n",
      "250 vectors processed\n",
      "300 vectors processed\n",
      "350 vectors processed\n",
      "400 vectors processed\n",
      "450 vectors processed\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# keyword extraction with gow #\n",
    "###############################\n",
    "\n",
    "#-------- gow\n",
    "\n",
    "keywords_gow = []  \n",
    "\n",
    "for counter,abstract in enumerate(abstracts_cleaned):\n",
    "    # create graph-of-words\n",
    "    g = terms_to_graph(abstract, w=4)\n",
    "    # decompose graph-of-words\n",
    "    core_numbers = dict(zip(g.vs['name'],g.coreness()))\n",
    "    # retain main core as keywords\n",
    "    max_c_n = max(core_numbers.values())\n",
    "    keywords = [kwd for kwd,c_n in core_numbers.items() if c_n==max_c_n]\n",
    "    # save results\n",
    "    keywords_gow.append(keywords)\n",
    "    \n",
    "    # print progress\n",
    "    if counter % round(len(abstracts_cleaned)/10) == 0:\n",
    "        print(counter, 'abstracts processed')\n",
    "\n",
    "#-------- gow_w\n",
    "        \n",
    "keywords_gow_w = []  \n",
    "\n",
    "\n",
    "for counter,abstract in enumerate(abstracts_cleaned):\n",
    "    \n",
    "    # create graph-of-words\n",
    "    g = terms_to_graph(abstract, w=4)\n",
    "    # decompose graph-of-words\n",
    "    core_numbers = weighted_core_dec(g)\n",
    "    # retain main core as keywords\n",
    "    max_c_n = max(core_numbers.values())\n",
    "    keywords = [kwd for kwd,c_n in core_numbers.items() if c_n==max_c_n]\n",
    "    # save results\n",
    "    keywords_gow_w.append(keywords)\n",
    "    \n",
    "    # print progress\n",
    "    if counter % round(len(abstracts_cleaned)/10) == 0:\n",
    "        print(counter, 'abstracts processed')\n",
    "\n",
    "#-------- TF-IDF\n",
    "my_percentage = 0.33\n",
    "\n",
    "# to ensure same pre-processing as the other methods\n",
    "abstracts_cleaned_strings = [' '.join(elt) for elt in abstracts_cleaned]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stpwds) # is stpwds necessary here?\n",
    "doc_term_matrix = tfidf_vectorizer.fit_transform(abstracts_cleaned_strings)\n",
    "\n",
    "### create an object 'terms' containing the column names of 'doc_term_matrix' ###\n",
    "### We will use the .get_feature_names() method ###\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "vectors_list = doc_term_matrix.todense().tolist()\n",
    "\n",
    "keywords_tfidf = []\n",
    "\n",
    "for counter,vector in enumerate(vectors_list):\n",
    "    # bow feature vector as list of tuples\n",
    "    terms_weights = zip(terms,vector)\n",
    "    # keep only non zero values (the words in the document)\n",
    "    \n",
    "    nonzero = [term for term in terms_weights if term[1] != 0]\n",
    "    # rank by decreasing weights\n",
    "    nonzero = sorted(nonzero, key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    # retain top 'my_percentage' words as keywords\n",
    "    numb_to_retain = int(len(nonzero)*my_percentage)\n",
    "    \n",
    "    keywords = [tuple[0] for tuple in nonzero[:numb_to_retain]]\n",
    "    \n",
    "    keywords_tfidf.append(keywords)\n",
    "    \n",
    "    # print progress\n",
    "    if counter % round(len(vectors_list)/10) == 0:\n",
    "        print(counter, 'vectors processed')\n",
    "\n",
    "#-------- PageRank\n",
    "keywords_pr = []\n",
    "\n",
    "for counter,abstract in enumerate(abstracts_cleaned):\n",
    "    ### fill the gaps ###\n",
    "    ### hint: combine the beginning of the gow loop with the middle section of the tfidf loop ###\n",
    "    # create graph-of-words\n",
    "    g = terms_to_graph(abstract, w=4)\n",
    "    # decompose graph-of-words\n",
    "    pr_scores = zip(g.vs['name'],g.pagerank())\n",
    "    pr_scores = sorted(pr_scores, key=operator.itemgetter(1), reverse=True)\n",
    "    # retain main core as keywords\n",
    "    numb_to_retain = int(len(pr_scores)*my_percentage)\n",
    "    keywords = [tuple[0] for tuple in pr_scores[:numb_to_retain]]\n",
    "\n",
    "    keywords_pr.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gow performance: \n",
      "\n",
      "precision: 0.5186082200000002\n",
      "recall: 0.6255520200000001\n",
      "F-1 score: 0.5154552000000003\n",
      "\n",
      "\n",
      "gow_w performance: \n",
      "\n",
      "precision: 0.4431831600000007\n",
      "recall: 0.8579845000000001\n",
      "F-1 score: 0.5663241799999997\n",
      "\n",
      "\n",
      "tfidf performance: \n",
      "\n",
      "precision: 0.5920879200000003\n",
      "recall: 0.3850372400000003\n",
      "F-1 score: 0.4485490400000005\n",
      "\n",
      "\n",
      "pr performance: \n",
      "\n",
      "precision: 0.6018220800000008\n",
      "recall: 0.3829783200000002\n",
      "F-1 score: 0.4496253800000002\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# performance evaluation #\n",
    "##########################\n",
    "\n",
    "perf_gow = [] \n",
    "perf_gow_w = []\n",
    "perf_tfidf = []\n",
    "perf_pr = []\n",
    "\n",
    "for idx, truth in enumerate(keywords_gold_standard):\n",
    "    perf_gow.append(accuracy_metrics(keywords_gow[idx], truth))\n",
    "    perf_gow_w.append(accuracy_metrics(keywords_gow_w[idx], truth))\n",
    "    perf_tfidf.append(accuracy_metrics(keywords_tfidf[idx], truth))\n",
    "    perf_pr.append(accuracy_metrics(keywords_pr[idx], truth))\n",
    "\n",
    "lkgs = len(keywords_gold_standard)\n",
    "\n",
    "\n",
    "# macro-averaged results (averaged at the collection level)\n",
    "\n",
    "results = {'gow':perf_gow,'gow_w':perf_gow_w,'tfidf':perf_tfidf,'pr':perf_pr}\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(name + ' performance: \\n')\n",
    "    print('precision:', sum([tuple[0] for tuple in result])/lkgs)\n",
    "    print('recall:', sum([tuple[1] for tuple in result])/lkgs)\n",
    "    print('F-1 score:', sum([tuple[2] for tuple in result])/lkgs)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algebra': 6, 'equat': 6, 'kind': 3, 'lambda': 6, 'linear': 6, 'm-dimension': 6, 'matric': 6, 'method': 6, 'numer': 4, 'solut': 6, 'special': 3, 'system': 6}\n",
      "-------------------------------------------------\n",
      "['algebra', 'equat', 'lambda', 'linear', 'm-dimension', 'matric', 'method', 'solut', 'system']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stpwds = stopwords.words('english')\n",
    "punct = string.punctuation.replace('-', '')\n",
    "\n",
    "my_doc = '''A method for solution of systems of linear algebraic equations \n",
    "with m-dimensional lambda matrices. A system of linear algebraic \n",
    "equations with m-dimensional lambda matrices is considered. \n",
    "The proposed method of searching for the solution of this system \n",
    "lies in reducing it to a numerical system of a special kind.'''\n",
    "\n",
    "my_doc = my_doc.replace('\\n', '')\n",
    "\n",
    "# pre-process document\n",
    "my_tokens = clean_text_simple(my_doc,my_stopwords=stpwds,punct=punct)\n",
    "                              \n",
    "g = terms_to_graph(my_tokens, w=4)\n",
    "  \n",
    "\n",
    "# decompose g\n",
    "core_numbers = unweighted_k_core(g)\n",
    "print(core_numbers)\n",
    "\n",
    "print('-------------------------------------------------')\n",
    "\n",
    "# retain main core as keywords\n",
    "max_c_n = max(core_numbers.values())\n",
    "keywords = [kwd for kwd, c_n in core_numbers.items() if c_n == max_c_n]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
