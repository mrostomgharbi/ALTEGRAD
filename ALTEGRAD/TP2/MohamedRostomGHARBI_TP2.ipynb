{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import igraph\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_to_graph(lists_of_terms, window_size, overspanning):\n",
    "    '''This function returns a directed, weighted igraph from lists of list of terms (the tokens from the pre-processed text)\n",
    "    e.g., [['quick','brown','fox'], ['develop', 'remot', 'control'], etc]\n",
    "    Edges are weighted based on term co-occurence within a sliding window of fixed size 'w' '''\n",
    "\n",
    "    if overspanning:\n",
    "        terms = [item for sublist in lists_of_terms for item in sublist]\n",
    "    else:\n",
    "        idx = 0\n",
    "        terms = lists_of_terms[idx]\n",
    "\n",
    "    from_to = {}\n",
    "\n",
    "    while True:\n",
    "        w = min(window_size, len(terms))\n",
    "        # create initial complete graph (first w terms)\n",
    "        terms_temp = terms[0:w]\n",
    "        indexes = list(itertools.combinations(range(w), r=2))\n",
    "\n",
    "        new_edges = []\n",
    "\n",
    "        for my_tuple in indexes:\n",
    "            new_edges.append(tuple([terms_temp[i] for i in my_tuple]))\n",
    "        for new_edge in new_edges:\n",
    "            if new_edge in from_to:\n",
    "                from_to[new_edge] += 1\n",
    "            else:\n",
    "                from_to[new_edge] = 1\n",
    "\n",
    "        # then iterate over the remaining terms\n",
    "        for i in range(w, len(terms)):\n",
    "            # term to consider\n",
    "            considered_term = terms[i]\n",
    "            # all terms within sliding window\n",
    "            terms_temp = terms[(i - w + 1):(i + 1)]\n",
    "\n",
    "            # edges to try\n",
    "            candidate_edges = []\n",
    "            for p in range(w - 1):\n",
    "                candidate_edges.append((terms_temp[p], considered_term))\n",
    "\n",
    "            for try_edge in candidate_edges:\n",
    "\n",
    "                # if not self-edge\n",
    "                if try_edge[1] != try_edge[0]:\n",
    "\n",
    "                    # if edge has already been seen, update its weight\n",
    "                    if try_edge in from_to:\n",
    "                        from_to[try_edge] += 1\n",
    "\n",
    "                    # if edge has never been seen, create it and assign it a unit weight\n",
    "                    else:\n",
    "                        from_to[try_edge] = 1\n",
    "\n",
    "        if overspanning:\n",
    "            break\n",
    "        else:\n",
    "            idx += 1\n",
    "            if idx == len(lists_of_terms):\n",
    "                break\n",
    "            terms = lists_of_terms[idx]\n",
    "\n",
    "    # create empty graph\n",
    "    g = igraph.Graph(directed=True)\n",
    "\n",
    "    # add vertices\n",
    "    if overspanning:\n",
    "        g.add_vertices(sorted(set(terms)))\n",
    "    else:\n",
    "        g.add_vertices(sorted(set([item for sublist in lists_of_terms for item in sublist])))\n",
    "\n",
    "    # add edges, direction is preserved since the graph is directed\n",
    "    g.add_edges(list(from_to.keys()))\n",
    "\n",
    "    # set edge and vertice weights\n",
    "    g.es['weight'] = list(from_to.values())  # based on co-occurence within sliding window\n",
    "    g.vs['weight'] = g.strength(weights=list(from_to.values()))  # weighted degree\n",
    "\n",
    "    return (g)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node_centrality(graph):\n",
    "    # degree\n",
    "    degrees = graph.degree()\n",
    "    degrees = [round(float(degree)/(len(graph.vs)-1),5) for degree in degrees]\n",
    "\n",
    "    # weighted degree\n",
    "    ### fill the gap ### hint: use the .strength() method with 'weights' argument\n",
    "    w_degrees = graph.strength(weights = graph.es[\"weight\"])\n",
    "    w_degrees = [round(float(degree)/(len(graph.vs)-1),5) for degree in w_degrees]\n",
    "\n",
    "    # closeness\n",
    "    ### fill the gap ### hint: use the .closeness() method with 'normalized' argument set to True\n",
    "    closeness = graph.closeness(normalized = True)\n",
    "    closeness = [round(value,5) for value in closeness]\n",
    "\n",
    "    # weighted closeness\n",
    "    ### fill the gap ### hint: same as above, but with 'weights' argument\n",
    "    w_closeness = graph.closeness(normalized = True , weights = graph.es[\"weight\"])\n",
    "    w_closeness = [round(value,5) for value in w_closeness]\n",
    "\n",
    "    return(list(zip(graph.vs[\"name\"],degrees,w_degrees,closeness,w_closeness)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top10(feature_names, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    # coef stores the weights of each feature (in unique term), for each class\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (class_label,\" \".join(feature_names[j] for j in top10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bot10(feature_names, clf, class_labels):\n",
    "    \"\"\"Prints features with the lowest coefficient values, per class\"\"\"\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        bot10 = np.argsort(clf.coef_[i])[0:9]\n",
    "        print(\"%s: %s\" % (class_label,\" \".join(feature_names[j] for j in bot10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document classification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "import pandas as pd\n",
    "# from library import terms_to_graph, compute_node_centrality, print_top10, print_bot10\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcode\u001b[0m/  \u001b[01;34mdata\u001b[0m/  lab2_handout.pdf  MohamedRostomGHARBI_TP2.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2803, 2)\n",
      "(1396, 2)\n",
      "first five rows of training data:\n",
      "         0                                                  1\n",
      "0  student  brian comput scienc depart univers wisconsin d...\n",
      "1  student  denni swanson web page mail pop uki offic hour...\n",
      "2  faculty  russel impagliazzo depart comput scienc engin ...\n",
      "3  student  dave phd student depart comput scienc univers ...\n",
      "4  project  center lifelong learn design univers colorado ...\n",
      "first five rows of testing data:\n",
      "         0                                                  1\n",
      "0  student  eric homepag eric wei tsinghua physic fudan genet\n",
      "1   course  comput system perform evalu model new sept ass...\n",
      "2  student  home page comput scienc grad student ucsd work...\n",
      "3  student  toni web page toni face thing call toni studen...\n",
      "4   course  ec advanc comput architectur credit parallel a...\n",
      "removing 29 documents from training set\n",
      "(2774, 2)\n",
      "removing 20 documents from test set\n",
      "(1376, 2)\n",
      "number of observations per class:\n",
      "project : 335\n",
      "student : 1075\n",
      "course : 620\n",
      "faculty : 744\n",
      "storing terms from training documents as list of lists\n",
      "storing terms from test documents as list of lists\n",
      "min, max and average number of terms per document: 4 20628 134.09084354722424\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# data loading and preprocessing #\n",
    "##################################\n",
    "\n",
    "path_to_data = \"data\"\n",
    "\n",
    "train = pd.read_csv(path_to_data + \"/webkb-train-stemmed.txt\", header=None, delimiter=\"\\t\")\n",
    "print(train.shape)\n",
    "\n",
    "test = pd.read_csv(path_to_data + \"/webkb-test-stemmed.txt\", header=None, delimiter=\"\\t\")\n",
    "print(test.shape)\n",
    "\n",
    "# inspect head of data frames\n",
    "print(\"first five rows of training data:\")\n",
    "print(train.iloc[:5,:])\n",
    "\n",
    "print(\"first five rows of testing data:\")\n",
    "print(test.iloc[:5,:])\n",
    "\n",
    "# get index of empty (nan) and less than four words documents (for which a graph cannot be built)\n",
    "index_remove = [i for i in range(len(train.iloc[:,1])) if (train.iloc[i,1]!=train.iloc[i,1]) or ((train.iloc[i,1]==train.iloc[i,1])and(len(train.iloc[i,1].split(\" \"))<4))]\n",
    "\n",
    "# remove those documents\n",
    "print(\"removing\", len(index_remove), \"documents from training set\")\n",
    "train = train.drop(train.index[index_remove])\n",
    "print(train.shape)\n",
    "\n",
    "# repeat above steps for test set\n",
    "index_remove = [i for i in range(len(test.iloc[:,1])) if (test.iloc[i,1]!=test.iloc[i,1]) or ((test.iloc[i,1]==test.iloc[i,1])and(len(test.iloc[i,1].split(\" \"))<4))]\n",
    "print(\"removing\", len(index_remove), \"documents from test set\")\n",
    "test = test.drop(test.index[index_remove])\n",
    "print(test.shape)\n",
    "\n",
    "labels = train.iloc[:,0]\n",
    "unique_labels = list(set(labels))\n",
    "\n",
    "truth = test.iloc[:,0]\n",
    "unique_truth = list(set(truth))\n",
    "\n",
    "print(\"number of observations per class:\")\n",
    "for label in unique_labels:\n",
    "    print(label, \":\", len([temp for temp in labels if temp==label]))\n",
    "\n",
    "print(\"storing terms from training documents as list of lists\")\n",
    "terms_by_doc = np.array([document.split(\" \") for document in train.iloc[:,1]])\n",
    "n_terms_per_doc = [len(terms) for terms in terms_by_doc]\n",
    "\n",
    "print(\"storing terms from test documents as list of lists\")\n",
    "terms_by_doc_test = np.array([document.split(\" \") for document in test.iloc[:,1]])\n",
    "\n",
    "print(\"min, max and average number of terms per document:\", min(n_terms_per_doc), max(n_terms_per_doc), sum(n_terms_per_doc)/len(n_terms_per_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all terms in list\n",
    "all_terms = [terms_by_doc[i][j] for i in range(len(terms_by_doc)) for j in range(len(terms_by_doc[i]))]\n",
    "\n",
    "### fill the gap ### hint: flatten 'terms_by_doc' (you may use a list comprehension)\n",
    "\n",
    "# compute average number of terms\n",
    "avg_len = sum(n_terms_per_doc)/len(n_terms_per_doc)\n",
    "\n",
    "# unique terms\n",
    "all_unique_terms = list(set(all_terms))\n",
    "# store IDF values in dictionary\n",
    "terms_by_doc_sets = [set(elt) for elt in terms_by_doc]\n",
    "n_doc = len(labels)\n",
    "idf = dict(zip(all_unique_terms,[0]*len(all_unique_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tba': 0,\n",
       " 'autom': 0,\n",
       " 'bernardino': 0,\n",
       " 'racquetbal': 0,\n",
       " 'church': 0,\n",
       " 'destin': 0,\n",
       " 'tau': 0,\n",
       " 'ow': 0,\n",
       " 'bernard': 0,\n",
       " 'cow': 0,\n",
       " 'manner': 0,\n",
       " 'semest': 0,\n",
       " 'webcrawl': 0,\n",
       " 'crap': 0,\n",
       " 'keeper': 0,\n",
       " 'psychologist': 0,\n",
       " 'adopt': 0,\n",
       " 'size': 0,\n",
       " 'moment': 0,\n",
       " 'biswa': 0,\n",
       " 'smithsonian': 0,\n",
       " 'teer': 0,\n",
       " 'andrea': 0,\n",
       " 'tell': 0,\n",
       " 'microsystem': 0,\n",
       " 'ignit': 0,\n",
       " 'allevi': 0,\n",
       " 'hodg': 0,\n",
       " 'gouda': 0,\n",
       " 'bitmap': 0,\n",
       " 'identif': 0,\n",
       " 'plug': 0,\n",
       " 'dozen': 0,\n",
       " 'horror': 0,\n",
       " 'harsh': 0,\n",
       " 'march': 0,\n",
       " 'ic': 0,\n",
       " 'drawer': 0,\n",
       " 'girl': 0,\n",
       " 'multidimension': 0,\n",
       " 'enforc': 0,\n",
       " 'sigir': 0,\n",
       " 'jolla': 0,\n",
       " 'etzioni': 0,\n",
       " 'suif': 0,\n",
       " 'brief': 0,\n",
       " 'beam': 0,\n",
       " 'stop': 0,\n",
       " 'slide': 0,\n",
       " 'openstep': 0,\n",
       " 'market': 0,\n",
       " 'concret': 0,\n",
       " 'automobil': 0,\n",
       " 'umd': 0,\n",
       " 'reactor': 0,\n",
       " 'auto': 0,\n",
       " 'haa': 0,\n",
       " 'written': 0,\n",
       " 'fanci': 0,\n",
       " 'poll': 0,\n",
       " 'mo': 0,\n",
       " 'tick': 0,\n",
       " 'focus': 0,\n",
       " 'hamilton': 0,\n",
       " 'hypothesi': 0,\n",
       " 'isl': 0,\n",
       " 'chanc': 0,\n",
       " 'routin': 0,\n",
       " 'softbot': 0,\n",
       " 'overlap': 0,\n",
       " 'manuel': 0,\n",
       " 'smallest': 0,\n",
       " 'porter': 0,\n",
       " 'advisor': 0,\n",
       " 'possibl': 0,\n",
       " 'shape': 0,\n",
       " 'annouc': 0,\n",
       " 'glimps': 0,\n",
       " 'yale': 0,\n",
       " 'rate': 0,\n",
       " 'louisiana': 0,\n",
       " 'lar': 0,\n",
       " 'flaw': 0,\n",
       " 'boor': 0,\n",
       " 'antonio': 0,\n",
       " 'bruton': 0,\n",
       " 'constitu': 0,\n",
       " 'mach': 0,\n",
       " 'design': 0,\n",
       " 'portugues': 0,\n",
       " 'lesli': 0,\n",
       " 'neuroetholog': 0,\n",
       " 'surgeri': 0,\n",
       " 'formal': 0,\n",
       " 'dataflow': 0,\n",
       " 'cartoon': 0,\n",
       " 'religion': 0,\n",
       " 'deitel': 0,\n",
       " 'earthquak': 0,\n",
       " 'entir': 0,\n",
       " 'debbi': 0,\n",
       " 'univeristi': 0,\n",
       " 'compens': 0,\n",
       " 'fridai': 0,\n",
       " 'leader': 0,\n",
       " 'charli': 0,\n",
       " 'boltzmann': 0,\n",
       " 'dy': 0,\n",
       " 'tango': 0,\n",
       " 'domain': 0,\n",
       " 'aw': 0,\n",
       " 'neo': 0,\n",
       " 'draw': 0,\n",
       " 'boyer': 0,\n",
       " 'inria': 0,\n",
       " 'midnight': 0,\n",
       " 'nossdav': 0,\n",
       " 'cuinfo': 0,\n",
       " 'honesti': 0,\n",
       " 'neumann': 0,\n",
       " 'chicago': 0,\n",
       " 'rob': 0,\n",
       " 'meant': 0,\n",
       " 'la': 0,\n",
       " 'herbert': 0,\n",
       " 'schulzrinn': 0,\n",
       " 'checker': 0,\n",
       " 'grammar': 0,\n",
       " 'fuzzi': 0,\n",
       " 'inadequ': 0,\n",
       " 'commiss': 0,\n",
       " 'ultrasound': 0,\n",
       " 'condon': 0,\n",
       " 'bsd': 0,\n",
       " 'constructor': 0,\n",
       " 'basement': 0,\n",
       " 'neurocomput': 0,\n",
       " 'explicitli': 0,\n",
       " 'dmitri': 0,\n",
       " 'sen': 0,\n",
       " 'ramon': 0,\n",
       " 'chan': 0,\n",
       " 'erupt': 0,\n",
       " 'nlm': 0,\n",
       " 'der': 0,\n",
       " 'hunter': 0,\n",
       " 'russia': 0,\n",
       " 'biograph': 0,\n",
       " 'babak': 0,\n",
       " 'ramin': 0,\n",
       " 'enscript': 0,\n",
       " 'bach': 0,\n",
       " 'conceiv': 0,\n",
       " 'mercer': 0,\n",
       " 'falsafi': 0,\n",
       " 'jair': 0,\n",
       " 'groupwar': 0,\n",
       " 'led': 0,\n",
       " 'flip': 0,\n",
       " 'cmsc': 0,\n",
       " 'volum': 0,\n",
       " 'fell': 0,\n",
       " 'verifi': 0,\n",
       " 'arun': 0,\n",
       " 'grail': 0,\n",
       " 'steam': 0,\n",
       " 'mason': 0,\n",
       " 'lindlei': 0,\n",
       " 'mak': 0,\n",
       " 'hpcd': 0,\n",
       " 'pami': 0,\n",
       " 'encourag': 0,\n",
       " 'cet': 0,\n",
       " 'psychiatr': 0,\n",
       " 'pronounc': 0,\n",
       " 'hua': 0,\n",
       " 'acoust': 0,\n",
       " 'qo': 0,\n",
       " 'compli': 0,\n",
       " 'evalu': 0,\n",
       " 'gari': 0,\n",
       " 'campbel': 0,\n",
       " 'penal': 0,\n",
       " 'petri': 0,\n",
       " 'colloquia': 0,\n",
       " 'decstat': 0,\n",
       " 'lifelong': 0,\n",
       " 'qualiti': 0,\n",
       " 'nearbi': 0,\n",
       " 'unpredict': 0,\n",
       " 'mcauliff': 0,\n",
       " 'forecast': 0,\n",
       " 'auditor': 0,\n",
       " 'arora': 0,\n",
       " 'grzegorz': 0,\n",
       " 'wagner': 0,\n",
       " 'davi': 0,\n",
       " 'bristol': 0,\n",
       " 'shirt': 0,\n",
       " 'polici': 0,\n",
       " 'batteri': 0,\n",
       " 'wave': 0,\n",
       " 'yap': 0,\n",
       " 'cry': 0,\n",
       " 'significantli': 0,\n",
       " 'jennif': 0,\n",
       " 'www': 0,\n",
       " 'box': 0,\n",
       " 'implementor': 0,\n",
       " 'buchanan': 0,\n",
       " 'dsmith': 0,\n",
       " 'agenda': 0,\n",
       " 'mood': 0,\n",
       " 'fink': 0,\n",
       " 'regard': 0,\n",
       " 'ecolog': 0,\n",
       " 'infoseek': 0,\n",
       " 'bring': 0,\n",
       " 'barri': 0,\n",
       " 'withdraw': 0,\n",
       " 'anaheim': 0,\n",
       " 'apm': 0,\n",
       " 'royal': 0,\n",
       " 'adjunct': 0,\n",
       " 'retim': 0,\n",
       " 'represent': 0,\n",
       " 'geo': 0,\n",
       " 'dod': 0,\n",
       " 'shah': 0,\n",
       " 'scientif': 0,\n",
       " 'rid': 0,\n",
       " 'yuan': 0,\n",
       " 'boardman': 0,\n",
       " 'noam': 0,\n",
       " 'breath': 0,\n",
       " 'guru': 0,\n",
       " 'octob': 0,\n",
       " 'william': 0,\n",
       " 'jacki': 0,\n",
       " 'lab': 0,\n",
       " 'boi': 0,\n",
       " 'tune': 0,\n",
       " 'lgrc': 0,\n",
       " 'elena': 0,\n",
       " 'zoom': 0,\n",
       " 'wolf': 0,\n",
       " 'quinn': 0,\n",
       " 'suni': 0,\n",
       " 'ashraf': 0,\n",
       " 'arbor': 0,\n",
       " 'armi': 0,\n",
       " 'umc': 0,\n",
       " 'christ': 0,\n",
       " 'english': 0,\n",
       " 'internet': 0,\n",
       " 'stl': 0,\n",
       " 'elabor': 0,\n",
       " 'clayton': 0,\n",
       " 'anneal': 0,\n",
       " 'tile': 0,\n",
       " 'checklist': 0,\n",
       " 'node': 0,\n",
       " 'sale': 0,\n",
       " 'explor': 0,\n",
       " 'de': 0,\n",
       " 'latex': 0,\n",
       " 'staff': 0,\n",
       " 'harold': 0,\n",
       " 'brand': 0,\n",
       " 'finish': 0,\n",
       " 'noteworthi': 0,\n",
       " 'strang': 0,\n",
       " 'globe': 0,\n",
       " 'behavior': 0,\n",
       " 'northwestern': 0,\n",
       " 'prepar': 0,\n",
       " 'weslei': 0,\n",
       " 'austria': 0,\n",
       " 'solowai': 0,\n",
       " 'errata': 0,\n",
       " 'wang': 0,\n",
       " 'chou': 0,\n",
       " 'asap': 0,\n",
       " 'worst': 0,\n",
       " 'seri': 0,\n",
       " 'autonom': 0,\n",
       " 'pollux': 0,\n",
       " 'reaction': 0,\n",
       " 'iie': 0,\n",
       " 'otter': 0,\n",
       " 'banner': 0,\n",
       " 'ipc': 0,\n",
       " 'sigcaph': 0,\n",
       " 'mcgill': 0,\n",
       " 'quarterli': 0,\n",
       " 'powel': 0,\n",
       " 'packer': 0,\n",
       " 'matroid': 0,\n",
       " 'string': 0,\n",
       " 'paragraph': 0,\n",
       " 'fond': 0,\n",
       " 'save': 0,\n",
       " 'nuprl': 0,\n",
       " 'mcgraw': 0,\n",
       " 'cop': 0,\n",
       " 'complet': 0,\n",
       " 'mage': 0,\n",
       " 'store': 0,\n",
       " 'afraid': 0,\n",
       " 'bigot': 0,\n",
       " 'addr': 0,\n",
       " 'length': 0,\n",
       " 'seventh': 0,\n",
       " 'voic': 0,\n",
       " 'newer': 0,\n",
       " 'reid': 0,\n",
       " 'tzu': 0,\n",
       " 'pipelin': 0,\n",
       " 'ncsu': 0,\n",
       " 'usenix': 0,\n",
       " 'cabl': 0,\n",
       " 'daniel': 0,\n",
       " 'norman': 0,\n",
       " 'athen': 0,\n",
       " 'fateman': 0,\n",
       " 'pentium': 0,\n",
       " 'interprocess': 0,\n",
       " 'paulb': 0,\n",
       " 'jersei': 0,\n",
       " 'red': 0,\n",
       " 'georg': 0,\n",
       " 'protocol': 0,\n",
       " 'oldest': 0,\n",
       " 'term': 0,\n",
       " 'unbeliev': 0,\n",
       " 'bland': 0,\n",
       " 'univ': 0,\n",
       " 'automata': 0,\n",
       " 'euclid': 0,\n",
       " 'henri': 0,\n",
       " 'club': 0,\n",
       " 'disambigu': 0,\n",
       " 'wart': 0,\n",
       " 'suburb': 0,\n",
       " 'maguir': 0,\n",
       " 'connectionist': 0,\n",
       " 'merg': 0,\n",
       " 'fix': 0,\n",
       " 'collector': 0,\n",
       " 'anymor': 0,\n",
       " 'regist': 0,\n",
       " 'ali': 0,\n",
       " 'credibl': 0,\n",
       " 'ret': 0,\n",
       " 'git': 0,\n",
       " 'fold': 0,\n",
       " 'mok': 0,\n",
       " 'journal': 0,\n",
       " 'kennedi': 0,\n",
       " 'episod': 0,\n",
       " 'panel': 0,\n",
       " 'remov': 0,\n",
       " 'talluri': 0,\n",
       " 'queen': 0,\n",
       " 'jie': 0,\n",
       " 'envis': 0,\n",
       " 'left': 0,\n",
       " 'ida': 0,\n",
       " 'bottl': 0,\n",
       " 'grosz': 0,\n",
       " 'dissert': 0,\n",
       " 'miscellani': 0,\n",
       " 'cchin': 0,\n",
       " 'dealt': 0,\n",
       " 'affili': 0,\n",
       " 'peek': 0,\n",
       " 'munich': 0,\n",
       " 'adder': 0,\n",
       " 'scheme': 0,\n",
       " 'carleton': 0,\n",
       " 'psych': 0,\n",
       " 'hay': 0,\n",
       " 'observ': 0,\n",
       " 'kata': 0,\n",
       " 'binari': 0,\n",
       " 'belfast': 0,\n",
       " 'kinda': 0,\n",
       " 'tricki': 0,\n",
       " 'regent': 0,\n",
       " 'microwav': 0,\n",
       " 'ward': 0,\n",
       " 'legisl': 0,\n",
       " 'ren': 0,\n",
       " 'kyle': 0,\n",
       " 'nec': 0,\n",
       " 'shoulder': 0,\n",
       " 'substructur': 0,\n",
       " 'vice': 0,\n",
       " 'iter': 0,\n",
       " 'boot': 0,\n",
       " 'instanc': 0,\n",
       " 'preliminari': 0,\n",
       " 'diagnosi': 0,\n",
       " 'deserv': 0,\n",
       " 'crime': 0,\n",
       " 'attach': 0,\n",
       " 'winsock': 0,\n",
       " 'thrown': 0,\n",
       " 'hebrew': 0,\n",
       " 'techreport': 0,\n",
       " 'letterman': 0,\n",
       " 'arian': 0,\n",
       " 'ernst': 0,\n",
       " 'amanda': 0,\n",
       " 'evanston': 0,\n",
       " 'tediou': 0,\n",
       " 'den': 0,\n",
       " 'fellowship': 0,\n",
       " 'stai': 0,\n",
       " 'antoni': 0,\n",
       " 'bunch': 0,\n",
       " 'grand': 0,\n",
       " 'garrett': 0,\n",
       " 'robotman': 0,\n",
       " 'crow': 0,\n",
       " 'freshman': 0,\n",
       " 'font': 0,\n",
       " 'deschut': 0,\n",
       " 'tamilnadu': 0,\n",
       " 'build': 0,\n",
       " 'counsel': 0,\n",
       " 'sarita': 0,\n",
       " 'refin': 0,\n",
       " 'whisker': 0,\n",
       " 'professorship': 0,\n",
       " 'kuo': 0,\n",
       " 'deploi': 0,\n",
       " 'isomorph': 0,\n",
       " 'dramat': 0,\n",
       " 'die': 0,\n",
       " 'forbidden': 0,\n",
       " 'weekli': 0,\n",
       " 'vijai': 0,\n",
       " 'soni': 0,\n",
       " 'bold': 0,\n",
       " 'aiken': 0,\n",
       " 'let': 0,\n",
       " 'achiev': 0,\n",
       " 'sustain': 0,\n",
       " 'overhead': 0,\n",
       " 'navig': 0,\n",
       " 'opaqu': 0,\n",
       " 'town': 0,\n",
       " 'costli': 0,\n",
       " 'phd': 0,\n",
       " 'tsai': 0,\n",
       " 'lagrangian': 0,\n",
       " 'omit': 0,\n",
       " 'henderson': 0,\n",
       " 'ousterhout': 0,\n",
       " 'speech': 0,\n",
       " 'mutual': 0,\n",
       " 'bag': 0,\n",
       " 'ben': 0,\n",
       " 'deros': 0,\n",
       " 'makeup': 0,\n",
       " 'cologn': 0,\n",
       " 'ascii': 0,\n",
       " 'top': 0,\n",
       " 'exploratori': 0,\n",
       " 'year': 0,\n",
       " 'riversid': 0,\n",
       " 'co': 0,\n",
       " 'cook': 0,\n",
       " 'trier': 0,\n",
       " 'hough': 0,\n",
       " 'nacho': 0,\n",
       " 'clear': 0,\n",
       " 'portion': 0,\n",
       " 'termin': 0,\n",
       " 'holidai': 0,\n",
       " 'suffici': 0,\n",
       " 'solicit': 0,\n",
       " 'stumbl': 0,\n",
       " 'laboratori': 0,\n",
       " 'importantli': 0,\n",
       " 'soft': 0,\n",
       " 'ftc': 0,\n",
       " 'mac': 0,\n",
       " 'victoria': 0,\n",
       " 'filesystem': 0,\n",
       " 'margo': 0,\n",
       " 'bart': 0,\n",
       " 'sclaroff': 0,\n",
       " 'ratliff': 0,\n",
       " 'hilton': 0,\n",
       " 'decad': 0,\n",
       " 'fase': 0,\n",
       " 'infer': 0,\n",
       " 'knee': 0,\n",
       " 'chee': 0,\n",
       " 'mainfram': 0,\n",
       " 'sundai': 0,\n",
       " 'convolut': 0,\n",
       " 'speaker': 0,\n",
       " 'revers': 0,\n",
       " 'michael': 0,\n",
       " 'spot': 0,\n",
       " 'op': 0,\n",
       " 'malik': 0,\n",
       " 'robbin': 0,\n",
       " 'sunris': 0,\n",
       " 'bloom': 0,\n",
       " 'schapiro': 0,\n",
       " 'automat': 0,\n",
       " 'schlumberg': 0,\n",
       " 'angri': 0,\n",
       " 'rpi': 0,\n",
       " 'welfar': 0,\n",
       " 'gottlieb': 0,\n",
       " 'agent': 0,\n",
       " 'val': 0,\n",
       " 'phenomenolog': 0,\n",
       " 'kramer': 0,\n",
       " 'snowboard': 0,\n",
       " 'sieg': 0,\n",
       " 'cshrc': 0,\n",
       " 'landau': 0,\n",
       " 'rice': 0,\n",
       " 'cscw': 0,\n",
       " 'disciplin': 0,\n",
       " 'radiu': 0,\n",
       " 'refus': 0,\n",
       " 'killer': 0,\n",
       " 'csli': 0,\n",
       " 'medina': 0,\n",
       " 'magnet': 0,\n",
       " 'columbu': 0,\n",
       " 'sole': 0,\n",
       " 'orsa': 0,\n",
       " 'spaa': 0,\n",
       " 'bui': 0,\n",
       " 'cummington': 0,\n",
       " 'sethi': 0,\n",
       " 'kushmerick': 0,\n",
       " 'avl': 0,\n",
       " 'london': 0,\n",
       " 'caution': 0,\n",
       " 'pearl': 0,\n",
       " 'parser': 0,\n",
       " 'almaden': 0,\n",
       " 'sharewar': 0,\n",
       " 'paradyn': 0,\n",
       " 'chronicl': 0,\n",
       " 'similar': 0,\n",
       " 'bicycl': 0,\n",
       " 'northeast': 0,\n",
       " 'pizza': 0,\n",
       " 'fine': 0,\n",
       " 'cute': 0,\n",
       " 'proce': 0,\n",
       " 'raft': 0,\n",
       " 'copi': 0,\n",
       " 'consortium': 0,\n",
       " 'master': 0,\n",
       " 'underpin': 0,\n",
       " 'junk': 0,\n",
       " 'csc': 0,\n",
       " 'salli': 0,\n",
       " 'aol': 0,\n",
       " 'griffin': 0,\n",
       " 'stream': 0,\n",
       " 'conrad': 0,\n",
       " 'matur': 0,\n",
       " 'faint': 0,\n",
       " 'fire': 0,\n",
       " 'twelfth': 0,\n",
       " 'haskel': 0,\n",
       " 'sigmod': 0,\n",
       " 'fyi': 0,\n",
       " 'consumm': 0,\n",
       " 'dine': 0,\n",
       " 'wong': 0,\n",
       " 'longer': 0,\n",
       " 'eugen': 0,\n",
       " 'siff': 0,\n",
       " 'guadalup': 0,\n",
       " 'brigg': 0,\n",
       " 'feng': 0,\n",
       " 'liaison': 0,\n",
       " 'sutton': 0,\n",
       " 'line': 0,\n",
       " 'median': 0,\n",
       " 'suspect': 0,\n",
       " 'miniatur': 0,\n",
       " 'alg': 0,\n",
       " 'freder': 0,\n",
       " 'shame': 0,\n",
       " 'nikolao': 0,\n",
       " 'school': 0,\n",
       " 'mip': 0,\n",
       " 'caen': 0,\n",
       " 'mpi': 0,\n",
       " 'upstair': 0,\n",
       " 'thoma': 0,\n",
       " 'invari': 0,\n",
       " 'blais': 0,\n",
       " 'feasibl': 0,\n",
       " 'slight': 0,\n",
       " 'corpu': 0,\n",
       " 'ijcai': 0,\n",
       " 'mention': 0,\n",
       " 'aeronaut': 0,\n",
       " 'darpa': 0,\n",
       " 'quizz': 0,\n",
       " 'mwf': 0,\n",
       " 'submiss': 0,\n",
       " 'censorship': 0,\n",
       " 'verlag': 0,\n",
       " 'maria': 0,\n",
       " 'destroi': 0,\n",
       " 'plot': 0,\n",
       " 'catalog': 0,\n",
       " 'poster': 0,\n",
       " 'neurolog': 0,\n",
       " 'mate': 0,\n",
       " 'plenti': 0,\n",
       " 'in': 0,\n",
       " 'eigenvector': 0,\n",
       " 'fitzpatrick': 0,\n",
       " 'tuft': 0,\n",
       " 'neal': 0,\n",
       " 'bore': 0,\n",
       " 'burrough': 0,\n",
       " 'enterpris': 0,\n",
       " 'hudson': 0,\n",
       " 'separ': 0,\n",
       " 'allegro': 0,\n",
       " 'organiz': 0,\n",
       " 'friend': 0,\n",
       " 'provid': 0,\n",
       " 'ethernet': 0,\n",
       " 'benefit': 0,\n",
       " 'trick': 0,\n",
       " 'export': 0,\n",
       " 'ravindra': 0,\n",
       " 'jane': 0,\n",
       " 'perceiv': 0,\n",
       " 'anchor': 0,\n",
       " 'odmg': 0,\n",
       " 'dec': 0,\n",
       " 'kurt': 0,\n",
       " 'tour': 0,\n",
       " 'dwarkada': 0,\n",
       " 'discard': 0,\n",
       " 'insight': 0,\n",
       " 'element': 0,\n",
       " 'hotjava': 0,\n",
       " 'direct': 0,\n",
       " 'subtre': 0,\n",
       " 'triangul': 0,\n",
       " 'polynomi': 0,\n",
       " 'analysi': 0,\n",
       " 'roger': 0,\n",
       " 'eric': 0,\n",
       " 'fabric': 0,\n",
       " 'flow': 0,\n",
       " 'disnei': 0,\n",
       " 'warm': 0,\n",
       " 'corba': 0,\n",
       " 'tag': 0,\n",
       " 'dewan': 0,\n",
       " 'educ': 0,\n",
       " 'batch': 0,\n",
       " 'assaf': 0,\n",
       " 'inter': 0,\n",
       " 'cach': 0,\n",
       " 'boulder': 0,\n",
       " 'ficu': 0,\n",
       " 'ac': 0,\n",
       " 'curri': 0,\n",
       " 'strong': 0,\n",
       " 'biomed': 0,\n",
       " 'amza': 0,\n",
       " 'makefil': 0,\n",
       " 'penn': 0,\n",
       " 'testabl': 0,\n",
       " 'alberta': 0,\n",
       " 'parti': 0,\n",
       " 'websit': 0,\n",
       " 'wahab': 0,\n",
       " 'result': 0,\n",
       " 'select': 0,\n",
       " 'robustli': 0,\n",
       " 'undo': 0,\n",
       " 'greenwood': 0,\n",
       " 'sparcstat': 0,\n",
       " 'constitut': 0,\n",
       " 'fault': 0,\n",
       " 'sunset': 0,\n",
       " 'pingali': 0,\n",
       " 'look': 0,\n",
       " 'griswold': 0,\n",
       " 'state': 0,\n",
       " 'seminar': 0,\n",
       " 'interrupt': 0,\n",
       " 'savag': 0,\n",
       " 'bright': 0,\n",
       " 'eth': 0,\n",
       " 'jonathan': 0,\n",
       " 'fisher': 0,\n",
       " 'coffe': 0,\n",
       " 'jess': 0,\n",
       " 'sutherland': 0,\n",
       " 'contract': 0,\n",
       " 'pressur': 0,\n",
       " 'directori': 0,\n",
       " 'cnd': 0,\n",
       " 'mount': 0,\n",
       " 'basu': 0,\n",
       " 'compliant': 0,\n",
       " 'wisc': 0,\n",
       " 'nation': 0,\n",
       " 'jone': 0,\n",
       " 'nyc': 0,\n",
       " 'replai': 0,\n",
       " 'founder': 0,\n",
       " 'kate': 0,\n",
       " 'sam': 0,\n",
       " 'conflict': 0,\n",
       " 'mental': 0,\n",
       " 'outdat': 0,\n",
       " 'decent': 0,\n",
       " 'yaser': 0,\n",
       " 'shasha': 0,\n",
       " 'dead': 0,\n",
       " 'field': 0,\n",
       " 'mesh': 0,\n",
       " 'ambigu': 0,\n",
       " 'hanoi': 0,\n",
       " 'salehi': 0,\n",
       " 'accomplish': 0,\n",
       " 'candid': 0,\n",
       " 'hui': 0,\n",
       " 'particl': 0,\n",
       " 'geometr': 0,\n",
       " 'lam': 0,\n",
       " 'segment': 0,\n",
       " 'acta': 0,\n",
       " 'resort': 0,\n",
       " 'egger': 0,\n",
       " 'lie': 0,\n",
       " 'duda': 0,\n",
       " 'younger': 0,\n",
       " 'odyssea': 0,\n",
       " 'krishna': 0,\n",
       " 'stock': 0,\n",
       " 'supplementari': 0,\n",
       " 'artifici': 0,\n",
       " 're': 0,\n",
       " 'magazin': 0,\n",
       " 'durham': 0,\n",
       " 'gist': 0,\n",
       " 'orthodox': 0,\n",
       " 'corner': 0,\n",
       " 'plai': 0,\n",
       " 'law': 0,\n",
       " 'fddi': 0,\n",
       " 'huge': 0,\n",
       " 'stress': 0,\n",
       " 'terrenc': 0,\n",
       " 'correl': 0,\n",
       " 'frog': 0,\n",
       " 'buse': 0,\n",
       " 'hobbi': 0,\n",
       " 'mitchel': 0,\n",
       " 'login': 0,\n",
       " 'marin': 0,\n",
       " 'professor': 0,\n",
       " 'von': 0,\n",
       " 'celebr': 0,\n",
       " 'badrinath': 0,\n",
       " 'jame': 0,\n",
       " 'understood': 0,\n",
       " 'msm': 0,\n",
       " 'nyu': 0,\n",
       " 'outer': 0,\n",
       " 'monei': 0,\n",
       " 'rapid': 0,\n",
       " 'ebel': 0,\n",
       " 'brownfield': 0,\n",
       " 'kluwer': 0,\n",
       " 'substanc': 0,\n",
       " 'character': 0,\n",
       " 'past': 0,\n",
       " 'fussel': 0,\n",
       " 'autonomi': 0,\n",
       " 'xlib': 0,\n",
       " 'keho': 0,\n",
       " 'charter': 0,\n",
       " 'propag': 0,\n",
       " 'doit': 0,\n",
       " 'slice': 0,\n",
       " 'zippel': 0,\n",
       " 'hierarchi': 0,\n",
       " 'annex': 0,\n",
       " 'wall': 0,\n",
       " 'deliv': 0,\n",
       " 'korea': 0,\n",
       " 'indiaworld': 0,\n",
       " 'abil': 0,\n",
       " 'telerobot': 0,\n",
       " 'superior': 0,\n",
       " 'qd': 0,\n",
       " 'intuitionist': 0,\n",
       " 'calculi': 0,\n",
       " 'brazil': 0,\n",
       " 'alexandr': 0,\n",
       " 'michel': 0,\n",
       " 'infom': 0,\n",
       " 'corequisit': 0,\n",
       " 'traub': 0,\n",
       " 'swap': 0,\n",
       " 'appoint': 0,\n",
       " 'melodi': 0,\n",
       " 'header': 0,\n",
       " 'infocom': 0,\n",
       " 'likewis': 0,\n",
       " 'tent': 0,\n",
       " 'unpublish': 0,\n",
       " 'rip': 0,\n",
       " 'readili': 0,\n",
       " 'data': 0,\n",
       " 'complaint': 0,\n",
       " 'digest': 0,\n",
       " 'invas': 0,\n",
       " 'architectur': 0,\n",
       " 'feder': 0,\n",
       " 'descend': 0,\n",
       " 'directli': 0,\n",
       " 'gail': 0,\n",
       " 'hospit': 0,\n",
       " 'kick': 0,\n",
       " 'grai': 0,\n",
       " 'netscap': 0,\n",
       " 'chow': 0,\n",
       " 'unnecessari': 0,\n",
       " 'videotap': 0,\n",
       " 'surfac': 0,\n",
       " 'late': 0,\n",
       " 'woodwork': 0,\n",
       " 'kernel': 0,\n",
       " 'noel': 0,\n",
       " 'pano': 0,\n",
       " 'helper': 0,\n",
       " 'ussr': 0,\n",
       " 'stretch': 0,\n",
       " 'amp': 0,\n",
       " 'instruct': 0,\n",
       " 'ubiquit': 0,\n",
       " 'teng': 0,\n",
       " 'sophist': 0,\n",
       " 'feed': 0,\n",
       " 'ground': 0,\n",
       " 'ensu': 0,\n",
       " 'domin': 0,\n",
       " 'multiresolut': 0,\n",
       " 'editor': 0,\n",
       " 'mse': 0,\n",
       " 'horwitz': 0,\n",
       " 'assum': 0,\n",
       " 'stratifi': 0,\n",
       " 'vendor': 0,\n",
       " 'split': 0,\n",
       " 'headlin': 0,\n",
       " 'oracl': 0,\n",
       " 'epsilon': 0,\n",
       " 'flight': 0,\n",
       " 'supervisor': 0,\n",
       " 'jami': 0,\n",
       " 'brett': 0,\n",
       " 'ture': 0,\n",
       " 'dsm': 0,\n",
       " 'therapi': 0,\n",
       " 'miron': 0,\n",
       " 'horu': 0,\n",
       " 'artif': 0,\n",
       " 'latin': 0,\n",
       " 'obviat': 0,\n",
       " 'cabot': 0,\n",
       " 'niagara': 0,\n",
       " 'gate': 0,\n",
       " 'anytim': 0,\n",
       " 'nifti': 0,\n",
       " 'advanc': 0,\n",
       " 'incid': 0,\n",
       " 'cheat': 0,\n",
       " 'nbc': 0,\n",
       " 'hail': 0,\n",
       " 'infopad': 0,\n",
       " 'rodriguez': 0,\n",
       " 'jain': 0,\n",
       " 'asplo': 0,\n",
       " 'calcul': 0,\n",
       " 'maker': 0,\n",
       " 'impair': 0,\n",
       " 'troi': 0,\n",
       " 'chair': 0,\n",
       " 'manifesto': 0,\n",
       " 'ohlrich': 0,\n",
       " 'ozsoyoglu': 0,\n",
       " 'deborah': 0,\n",
       " 'district': 0,\n",
       " 'gslocal': 0,\n",
       " 'product': 0,\n",
       " 'harvei': 0,\n",
       " 'linguist': 0,\n",
       " 'chernoff': 0,\n",
       " 'philipp': 0,\n",
       " 'televis': 0,\n",
       " 'wayn': 0,\n",
       " 'kit': 0,\n",
       " 'moss': 0,\n",
       " 'ariel': 0,\n",
       " 'calibr': 0,\n",
       " 'tulan': 0,\n",
       " 'interfac': 0,\n",
       " 'anthoni': 0,\n",
       " 'richard': 0,\n",
       " 'dtd': 0,\n",
       " 'lane': 0,\n",
       " 'pedro': 0,\n",
       " 'home': 0,\n",
       " 'introduc': 0,\n",
       " 'gain': 0,\n",
       " 'acceler': 0,\n",
       " 'encod': 0,\n",
       " 'summat': 0,\n",
       " 'turtl': 0,\n",
       " 'wichita': 0,\n",
       " 'request': 0,\n",
       " 'ori': 0,\n",
       " 'privat': 0,\n",
       " 'sweet': 0,\n",
       " 'duan': 0,\n",
       " 'stone': 0,\n",
       " 'acquisit': 0,\n",
       " 'share': 0,\n",
       " 'bboard': 0,\n",
       " 'umiac': 0,\n",
       " 'jeb': 0,\n",
       " 'ci': 0,\n",
       " 'keynot': 0,\n",
       " 'lnc': 0,\n",
       " 'battl': 0,\n",
       " 'langaug': 0,\n",
       " 'toil': 0,\n",
       " 'modifi': 0,\n",
       " 'vladimir': 0,\n",
       " 'radiolog': 0,\n",
       " 'hytim': 0,\n",
       " 'shell': 0,\n",
       " 'alias': 0,\n",
       " 'duli': 0,\n",
       " 'cpsc': 0,\n",
       " 'surgic': 0,\n",
       " 'adm': 0,\n",
       " 'cloud': 0,\n",
       " 'callawai': 0,\n",
       " 'opinion': 0,\n",
       " 'hire': 0,\n",
       " 'icpp': 0,\n",
       " 'download': 0,\n",
       " 'alabama': 0,\n",
       " 'commun': 0,\n",
       " 'blank': 0,\n",
       " 'intro': 0,\n",
       " 'hyplan': 0,\n",
       " 'transi': 0,\n",
       " 'applet': 0,\n",
       " 'chapter': 0,\n",
       " 'rocki': 0,\n",
       " 'pool': 0,\n",
       " 'gibson': 0,\n",
       " 'gui': 0,\n",
       " 'sharon': 0,\n",
       " 'ansi': 0,\n",
       " 'sitaraman': 0,\n",
       " 'rain': 0,\n",
       " 'arbitrarili': 0,\n",
       " 'command': 0,\n",
       " 'leav': 0,\n",
       " 'differenc': 0,\n",
       " 'list': 0,\n",
       " 'erich': 0,\n",
       " 'folk': 0,\n",
       " 'held': 0,\n",
       " 'utc': 0,\n",
       " 'quasi': 0,\n",
       " 'nomad': 0,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 terms processed\n",
      "1000 terms processed\n",
      "2000 terms processed\n",
      "3000 terms processed\n",
      "4000 terms processed\n",
      "5000 terms processed\n",
      "6000 terms processed\n",
      "7000 terms processed\n"
     ]
    }
   ],
   "source": [
    "for counter,unique_term in enumerate(list(idf.keys())):\n",
    "    # compute number of documents in which 'unique_term' appears\n",
    "    df =np.sum(np.array([unique_term in terms for terms in terms_by_doc_sets ])) ### fill the gap # \n",
    "    idf[unique_term] = math.log10((len(all_unique_terms)+1)/df)\n",
    "    if counter % 1e3 == 0:\n",
    "        print(counter, \"terms processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a graph-of-words for the collection\n",
      "True\n",
      "creating a graph-of-words for each training document\n",
      "True\n",
      "True\n",
      "computing vector representations of each training document\n",
      "0 documents processed\n",
      "1000 documents processed\n",
      "2000 documents processed\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# computing features for the training set #\n",
    "###########################################\n",
    "\n",
    "w = 3 # sliding window size\n",
    "\n",
    "print(\"creating a graph-of-words for the collection\")\n",
    "\n",
    "c_g = terms_to_graph(terms_by_doc , w , overspanning=False)### fill the gap ### hint: use the terms_to_graph function with the proper arguments\n",
    "\n",
    "# sanity check (should return True)\n",
    "print(len(all_unique_terms) == len(c_g.vs))\n",
    "\n",
    "print(\"creating a graph-of-words for each training document\")\n",
    "\n",
    "all_graphs = []\n",
    "for elt in terms_by_doc:\n",
    "    all_graphs.append(terms_to_graph([elt],w,overspanning=True))\n",
    "\n",
    "# sanity checks (should return True)\n",
    "print(len(terms_by_doc)==len(all_graphs))\n",
    "print(len(set(terms_by_doc[0]))==len(all_graphs[0].vs))\n",
    "\n",
    "print(\"computing vector representations of each training document\")\n",
    "\n",
    "b = 0.003\n",
    "\n",
    "features_degree = []\n",
    "features_w_degree = []\n",
    "features_closeness = []\n",
    "features_w_closeness = []\n",
    "features_twicw = [] # we try it only with unweighted degree\n",
    "features_tfidf = []\n",
    "\n",
    "len_all = len(all_unique_terms)\n",
    "collection_degrees = collection_degrees = dict(zip(c_g.vs['name'] , c_g.strength())) ### fill the gap ### hint: build a dict where the keys are the names of the nodes in the collection graph and the values are their unweighted degrees\n",
    "\n",
    "\n",
    "maxcol = max(list(collection_degrees.values()))\n",
    "\n",
    "for i, graph in enumerate(all_graphs):\n",
    "    \n",
    "    terms_in_doc = terms_by_doc[i]\n",
    "    doc_len = len(terms_in_doc)\n",
    "    \n",
    "    # returns node (0) name, (1) degree, (2) weighted degree, (3) closeness, (4) weighted closeness\n",
    "    my_metrics = compute_node_centrality(graph)\n",
    "    \n",
    "    feature_row_degree = [0]*len_all\n",
    "    feature_row_w_degree = [0]*len_all\n",
    "    feature_row_closeness = [0]*len_all\n",
    "    feature_row_w_closeness = [0]*len_all\n",
    "    feature_row_twicw = [0]*len_all\n",
    "    feature_row_tfidf = [0]*len_all\n",
    "    \n",
    "    # iterate over the unique terms contained by the doc (for all the other columns, the values will remain at zero)\n",
    "    for term in list(set(terms_in_doc)):\n",
    "        \n",
    "        index = all_unique_terms.index(term)\n",
    "        idf_term = idf[term]\n",
    "        denominator = (1-b+(b*(float(doc_len)/avg_len))) ### fill the gap ### hint: refer to the TF equation in the handout\n",
    "        metrics_term = [tuple[1:] for tuple in my_metrics if tuple[0]==term][0]\n",
    "        \n",
    "        # store TW-IDF values\n",
    "        feature_row_degree[index] = (metrics_term[0]/denominator) * idf_term\n",
    "        feature_row_w_degree[index] = (metrics_term[1]/denominator) * idf_term\n",
    "        feature_row_closeness[index] = (metrics_term[2]/denominator) * idf_term\n",
    "        feature_row_w_closeness[index] = (metrics_term[3]/denominator) * idf_term\n",
    "        \n",
    "        # store TW-ICW values\n",
    "        feature_row_twicw[index] = (metrics_term[0]/denominator) * math.log10((maxcol+1)/collection_degrees[term]) \n",
    "        \n",
    "        # number of occurences of word in document\n",
    "        tf = terms_in_doc.count(term)        \n",
    "        # store TF-IDF value\n",
    "        feature_row_tfidf[index] = ((1+math.log1p(1+math.log1p(tf)))/(1-0.2+(0.2*(float(doc_len)/avg_len)))) * idf_term\n",
    "    \n",
    "    features_degree.append(feature_row_degree)\n",
    "    features_w_degree.append(feature_row_w_degree)\n",
    "    features_closeness.append(feature_row_closeness)\n",
    "    features_w_closeness.append(feature_row_w_closeness)\n",
    "    features_twicw.append(feature_row_twicw)\n",
    "    features_tfidf.append(feature_row_tfidf)\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print (i, \"documents processed\")\n",
    "\n",
    "# convert list of lists into array\n",
    "# documents as rows, unique words (features) as columns\n",
    "training_set_degree = numpy.array(features_degree)\n",
    "training_set_w_degree = numpy.array(features_w_degree)\n",
    "training_set_closeness = numpy.array(features_closeness)\n",
    "training_set_w_closeness = numpy.array(features_w_closeness)\n",
    "training_set_tw_icw = numpy.array(features_twicw)\n",
    "training_set_tfidf = numpy.array(features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a graph-of-words for each test document\n",
      "True\n",
      "True\n",
      "computing vector representations of each test document\n",
      "0 documents processed\n",
      "500 documents processed\n",
      "1000 documents processed\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# computing features for the test set #\n",
    "#######################################\n",
    "\n",
    "print(\"creating a graph-of-words for each test document\")\n",
    "\n",
    "all_graphs_test = []\n",
    "for elt in terms_by_doc_test:\n",
    "    all_graphs_test.append(terms_to_graph([elt],w,overspanning=True))\n",
    "\n",
    "# sanity checks (should return True)\n",
    "print(len(terms_by_doc_test)==len(all_graphs_test))\n",
    "print(len(set(terms_by_doc_test[0]))==len(all_graphs_test[0].vs))\n",
    "\n",
    "print(\"computing vector representations of each test document\")\n",
    "# ! each test document is represented in the training space only\n",
    "\n",
    "features_degree_test = []\n",
    "features_w_degree_test = []\n",
    "features_closeness_test = []\n",
    "features_w_closeness_test = []\n",
    "features_twicw_test = []\n",
    "features_tfidf_test = []\n",
    "\n",
    "for i, graph in enumerate(all_graphs_test):\n",
    "    \n",
    "    # filter out the terms that are not in the training set\n",
    "    terms_in_doc = [term for term in terms_by_doc_test[i] if term in all_unique_terms]\n",
    "    doc_len = len(terms_in_doc)\n",
    "    \n",
    "    my_metrics = compute_node_centrality(graph)\n",
    "    \n",
    "    feature_row_degree_test = [0]*len_all\n",
    "    feature_row_w_degree_test = [0]*len_all\n",
    "    feature_row_closeness_test = [0]*len_all\n",
    "    feature_row_w_closeness_test = [0]*len_all\n",
    "    feature_row_twicw_test = [0]*len_all\n",
    "    feature_row_tfidf_test = [0]*len_all\n",
    "\n",
    "    for term in list(set(terms_in_doc)):\n",
    "        index = all_unique_terms.index(term)\n",
    "        idf_term = idf[term]\n",
    "        denominator = (1-b+(b*(float(doc_len)/avg_len)))\n",
    "        metrics_term = [tuple[1:] for tuple in my_metrics if tuple[0]==term][0]\n",
    "        \n",
    "        # store TW-IDF values      \n",
    "        feature_row_degree_test[index] = (metrics_term[0]/denominator) * idf_term\n",
    "        feature_row_w_degree_test[index] = (metrics_term[1]/denominator) * idf_term\n",
    "        feature_row_closeness_test[index] = (metrics_term[2]/denominator) * idf_term\n",
    "        feature_row_w_closeness_test[index] = (metrics_term[3]/denominator) * idf_term\n",
    "        \n",
    "        # store TW-ICW values\n",
    "        feature_row_twicw_test[index] = (metrics_term[0]/denominator) * (math.log10((maxcol+1)/collection_degrees[term]))\n",
    "\n",
    "        # number of occurences of word in document\n",
    "        tf = terms_in_doc.count(term)\n",
    "        # store TF-IDF value\n",
    "        feature_row_tfidf_test[index] = ((1+math.log1p(1+math.log1p(tf)))/(1-0.2+(0.2*(float(doc_len)/avg_len)))) * idf_term\n",
    "\n",
    "    features_degree_test.append(feature_row_degree_test)\n",
    "    features_w_degree_test.append(feature_row_w_degree_test)\n",
    "    features_closeness_test.append(feature_row_closeness_test)\n",
    "    features_w_closeness_test.append(feature_row_w_closeness_test)\n",
    "    features_twicw_test.append(feature_row_twicw_test)\n",
    "    features_tfidf_test.append(feature_row_tfidf_test)\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print (i, \"documents processed\")\n",
    "\n",
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., document-term matrix)\n",
    "testing_set_degree = numpy.array(features_degree_test)\n",
    "testing_set_w_degree = numpy.array(features_w_degree_test)\n",
    "testing_set_closeness = numpy.array(features_closeness_test)\n",
    "testing_set_w_closeness = numpy.array(features_w_closeness_test)\n",
    "testing_set_twicw = numpy.array(features_twicw_test)\n",
    "testing_set_tfidf = numpy.array(features_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'student'), (2, 'course'), (1, 'student'), (1, 'student'), (2, 'course'), (3, 'faculty'), (2, 'course'), (1, 'student'), (3, 'faculty'), (0, 'project'), (1, 'student'), (3, 'faculty'), (1, 'student'), (2, 'course'), (3, 'faculty'), (2, 'course'), (2, 'course'), (1, 'student'), (1, 'student'), (0, 'project')]\n",
      "[(1, 'student'), (1, 'student'), (3, 'faculty'), (1, 'student'), (0, 'project'), (3, 'faculty'), (3, 'faculty'), (3, 'faculty'), (1, 'student'), (2, 'course'), (1, 'student'), (3, 'faculty'), (1, 'student'), (1, 'student'), (3, 'faculty'), (3, 'faculty'), (3, 'faculty'), (1, 'student'), (3, 'faculty'), (1, 'student')]\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "# labels #\n",
    "##########\n",
    "\n",
    "# convert labels into integers then into column array\n",
    "labels = list(labels)\n",
    "labels_int = [0] * len(labels)\n",
    "for j in range(len(unique_labels)):\n",
    "    index_temp = [i for i in range(len(labels)) if labels[i]==unique_labels[j]]\n",
    "    for element in index_temp:\n",
    "        labels_int[element] = j\n",
    "        \n",
    "# convert truth into integers then into column array\n",
    "truth = list(truth)\n",
    "truth_int = [0] * len(truth)\n",
    "for j in range(len(unique_truth)):\n",
    "    index_temp = [i for i in range(len(truth)) if truth[i]==unique_truth[j]]\n",
    "    for element in index_temp:\n",
    "        truth_int[element] = j\n",
    "\n",
    "# check that coding went smoothly\n",
    "print(list(zip(truth_int,truth))[:20])\n",
    "\n",
    "truth_array = numpy.array(truth_int)\n",
    "\n",
    "# check that coding went smoothly\n",
    "print(list(zip(labels_int,labels))[:20])\n",
    "labels_array = numpy.array(labels_int)\n",
    "\n",
    "for clf in [\"LinearSVC\",\"LogisticRegression\",\"MultinomialNB\"]:\n",
    "    \n",
    "    if clf==\"LinearSVC\":\n",
    "        classifier_degree = svm.LinearSVC()\n",
    "        classifier_w_degree = svm.LinearSVC()\n",
    "        classifier_closeness = svm.LinearSVC()\n",
    "        classifier_w_closeness = svm.LinearSVC()\n",
    "        classifier_twicw = svm.LinearSVC()\n",
    "        classifier_tfidf = svm.LinearSVC()\n",
    "    elif clf==\"LogisticRegression\":\n",
    "        classifier_degree = LogisticRegression(multi_class='ovr',solver='liblinear') # we specify multi_class and solver arguments just to avoid getting a warning\n",
    "        classifier_w_degree = LogisticRegression(multi_class='ovr',solver='liblinear')\n",
    "        classifier_closeness = LogisticRegression(multi_class='ovr',solver='liblinear')\n",
    "        classifier_w_closeness = LogisticRegression(multi_class='ovr',solver='liblinear')\n",
    "        classifier_twicw = LogisticRegression(multi_class='ovr',solver='liblinear')\n",
    "        classifier_tfidf = LogisticRegression(multi_class='ovr',solver='liblinear')\n",
    "    elif clf==\"MultinomialNB\":\n",
    "        classifier_degree = MultinomialNB()\n",
    "        classifier_w_degree = MultinomialNB()\n",
    "        classifier_closeness = MultinomialNB()\n",
    "        classifier_w_closeness = MultinomialNB()\n",
    "        classifier_twicw = MultinomialNB()\n",
    "        classifier_tfidf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training MultinomialNB classifiers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "    ############\n",
    "    # training #\n",
    "    ############\n",
    "    \n",
    "    print(\"training\", clf, \"classifiers\")\n",
    "    classifier_degree.fit(training_set_degree, labels_array)\n",
    "    classifier_w_degree.fit(training_set_w_degree, labels_array)\n",
    "    classifier_closeness.fit(training_set_closeness, labels_array)\n",
    "    classifier_w_closeness.fit(training_set_w_closeness, labels_array)\n",
    "    classifier_twicw.fit(training_set_tw_icw, labels_array)\n",
    "    classifier_tfidf.fit(training_set_tfidf, labels_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== accuracy for MultinomialNB classifier ==========\n",
      "accuracy TW-IDF degree: 81.25\n",
      "accuracy TW-IDF weighted degree: 81.395\n",
      "accuracy TW-IDF closeness: 85.974\n",
      "accuracy TW-IDF weighted closeness: 86.337\n",
      "accuracy TW-ICW degree: 78.125\n",
      "accuracy TF-IDF: 85.174\n",
      "project: laboratori support faculti inform comput develop system group research project\n",
      "student: work graduat interest home depart page student comput scienc univers\n",
      "course: note lectur grade exam homework hour class syllabu instructor assign\n",
      "faculty: public depart associ interest scienc univers comput fax research professor\n"
     ]
    }
   ],
   "source": [
    "    ###########\n",
    "    # testing #\n",
    "    ###########\n",
    "    \n",
    "    # issue predictions\n",
    "    predictions_degree = classifier_degree.predict(testing_set_degree)\n",
    "    predictions_w_degree = classifier_w_degree.predict(testing_set_w_degree)\n",
    "    predictions_closeness = classifier_closeness.predict(testing_set_closeness)\n",
    "    predictions_w_closeness = classifier_w_closeness.predict(testing_set_w_closeness)\n",
    "    predictions_twicw = classifier_twicw.predict(testing_set_twicw)\n",
    "    predictions_tfidf = classifier_tfidf.predict(testing_set_tfidf)\n",
    "    \n",
    "    print('========== accuracy for', clf ,'classifier ==========')\n",
    "    print(\"accuracy TW-IDF degree:\", round(metrics.accuracy_score(truth_array,predictions_degree)*100,3))\n",
    "    print(\"accuracy TW-IDF weighted degree:\", round(metrics.accuracy_score(truth_array,predictions_w_degree)*100,3))\n",
    "    print(\"accuracy TW-IDF closeness:\", round(metrics.accuracy_score(truth_array,predictions_closeness)*100,3))\n",
    "    print(\"accuracy TW-IDF weighted closeness:\", round(metrics.accuracy_score(truth_array,predictions_w_closeness)*100,3))\n",
    "    print(\"accuracy TW-ICW degree:\", round(metrics.accuracy_score(truth_array,predictions_twicw)*100,3))\n",
    "    print(\"accuracy TF-IDF:\", round(metrics.accuracy_score(truth_array,predictions_tfidf)*100,3))\n",
    "    \n",
    "# show the most and less important features for each class\n",
    "\n",
    "\n",
    "### fill the gaps ### hint: pick a classifier (e.g., 'classifier_tfidf'), and pass it to the 'print_top10' and 'print_bot10' functions along with 'unique_labels' and 'all_unique_terms'\n",
    "print_top10(all_unique_terms , classifier_tfidf , unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "fr"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "fr",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
