{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq - Mohamed Rostom GHARBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.backend.tensorflow_backend import _to_tensor\n",
    "from keras.layers import Input, Embedding, Dropout, Bidirectional, GRU, TimeDistributed, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = ''\n",
    "path_to_data = path_root + 'data/'\n",
    "\n",
    "sys.path.insert(0, path_root)\n",
    "\n",
    "from AttentionWithContext import AttentionWithContext\n",
    "\n",
    "def bidir_gru(my_seq,n_units):\n",
    "    '''\n",
    "    just a convenient wrapper for bidirectional RNN with GRU units\n",
    "    '''\n",
    "    return Bidirectional(# adding a default GRU layer (https://keras.io/layers/recurrent/). \n",
    "                         GRU(units=n_units, activation='tanh', return_sequences=True),\n",
    "                         merge_mode='concat', weights=None)(my_seq)\n",
    " \n",
    "# = = = = = parameters = = = = =\n",
    "\n",
    "n_units = 50\n",
    "drop_rate = 0.5 \n",
    "mfw_idx = 2 # index of the most frequent words in the dictionary. \n",
    "            # 0 is for the special padding token\n",
    "            # 1 is for the special out-of-vocabulary token\n",
    "\n",
    "padding_idx = 0\n",
    "oov_idx = 1\n",
    "batch_size = 32\n",
    "nb_epochs = 6\n",
    "my_optimizer = 'adam'\n",
    "my_patience = 2 # for early stopping strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = data loading = = = = =\n",
    "\n",
    "my_docs_array_train = np.load(path_to_data + 'docs_train.npy')\n",
    "my_docs_array_test = np.load(path_to_data + 'docs_test.npy')\n",
    "\n",
    "my_labels_array_train = np.load(path_to_data + 'labels_train.npy')\n",
    "my_labels_array_test = np.load(path_to_data + 'labels_test.npy')\n",
    "\n",
    "# load dictionary of word indexes (sorted by decreasing frequency across the corpus)\n",
    "with open(path_to_data + 'word_to_index.json', 'r') as my_file:\n",
    "    word_to_index = json.load(my_file)\n",
    "\n",
    "# invert mapping\n",
    "index_to_word = dict((v,k) for k,v in word_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings created\n",
      "embeddings compressed\n"
     ]
    }
   ],
   "source": [
    "# = = = = = loading pretrained word vectors = = = = =\n",
    "\n",
    "wvs = KeyedVectors.load(path_to_data + 'word_vectors.kv', mmap='r')\n",
    "assert len(wvs.wv.vocab) == len(word_to_index) + 1 # vocab does not contain the OOV token\n",
    "\n",
    "word_vecs = wvs.wv.syn0\n",
    "\n",
    "pad_vec = np.random.normal(size=word_vecs.shape[1])\n",
    "\n",
    "# add Gaussian vector on top of embedding matrix (padding vector)\n",
    "word_vecs = np.insert(word_vecs,0,pad_vec,0)\n",
    "\n",
    "print('embeddings created')\n",
    "\n",
    "# reduce dimension with PCA (to reduce the number of parameters of the model)\n",
    "my_pca = PCA(n_components=64)\n",
    "embeddings_pca = my_pca.fit_transform(word_vecs)\n",
    "\n",
    "print('embeddings compressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model compiled\n"
     ]
    }
   ],
   "source": [
    "# = = = = = defining architecture = = = = =\n",
    "\n",
    "# = = = sentence encoder\n",
    "\n",
    "sent_ints = Input(shape=(my_docs_array_train.shape[2],)) # vec of ints of variable size\n",
    "\n",
    "sent_wv = Embedding(input_dim=embeddings_pca.shape[0], # vocab size\n",
    "                    output_dim=embeddings_pca.shape[1], # dimensionality of embedding space\n",
    "                    weights=[embeddings_pca],\n",
    "                    input_length=my_docs_array_train.shape[2],\n",
    "                    trainable=True\n",
    "                    )(sent_ints)\n",
    "\n",
    "sent_wv_dr = Dropout(drop_rate)(sent_wv)\n",
    "\n",
    "# using bidir_gru, AttentionWithContext with return_coefficients=True, and Dropout\n",
    "\n",
    "sent_wa = bidir_gru(sent_wv_dr , n_units)\n",
    "\n",
    "sent_att_vec , word_att_coeffs = AttentionWithContext(return_coefficients=True)(sent_wa)\n",
    "\n",
    "sent_att_vec_dr = Dropout(drop_rate)(sent_att_vec)\n",
    "\n",
    "sent_encoder = Model(sent_ints,sent_att_vec_dr)\n",
    "\n",
    "# = = = document encoder\n",
    "\n",
    "doc_ints = Input(shape=(my_docs_array_train.shape[1],my_docs_array_train.shape[2],))\n",
    "\n",
    "\n",
    "# using TimeDistributed (https://keras.io/layers/wrappers/), bidir_gru, AttentionWithContext with return_coefficients=True, and Dropout\n",
    "                  \n",
    "sent_att_vec_dr = TimeDistributed(sent_encoder)(doc_ints)\n",
    "\n",
    "doc_sa = bidir_gru(sent_att_vec_dr , n_units)\n",
    "\n",
    "doc_att_vec , sent_att_coeffs = AttentionWithContext(return_coefficients=True)(doc_sa)\n",
    "\n",
    "doc_att_vec_dr = Dropout(drop_rate)(doc_att_vec)\n",
    "\n",
    "preds = Dense(units=1,\n",
    "              activation='sigmoid')(doc_att_vec_dr)\n",
    "\n",
    "model = Model(doc_ints,preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer = my_optimizer,\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print('model compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/6\n",
      "25000/25000 [==============================] - 240s 10ms/step - loss: 0.5480 - acc: 0.7056 - val_loss: 0.4296 - val_acc: 0.7997\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.42964, saving model to data/model\n",
      "Epoch 2/6\n",
      "25000/25000 [==============================] - 227s 9ms/step - loss: 0.3631 - acc: 0.8428 - val_loss: 0.3741 - val_acc: 0.8350\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.42964 to 0.37406, saving model to data/model\n",
      "Epoch 3/6\n",
      "25000/25000 [==============================] - 232s 9ms/step - loss: 0.2809 - acc: 0.8844 - val_loss: 0.3685 - val_acc: 0.8403\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37406 to 0.36848, saving model to data/model\n",
      "Epoch 4/6\n",
      "25000/25000 [==============================] - 235s 9ms/step - loss: 0.2207 - acc: 0.9109 - val_loss: 0.3811 - val_acc: 0.8421\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36848\n",
      "Epoch 5/6\n",
      "25000/25000 [==============================] - 6181s 247ms/step - loss: 0.1833 - acc: 0.9290 - val_loss: 0.4082 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.36848\n",
      "Epoch 6/6\n",
      "25000/25000 [==============================] - 342s 14ms/step - loss: 0.1529 - acc: 0.9415 - val_loss: 0.4553 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.36848\n"
     ]
    }
   ],
   "source": [
    "# = = = = = training = = = = =\n",
    "\n",
    "loading_pretrained = False\n",
    "\n",
    "if not loading_pretrained:\n",
    "    early_stopping = EarlyStopping(monitor='val_acc', # go through epochs as long as accuracy on validation set increases\n",
    "                                   patience=my_patience,\n",
    "                                   mode='max')\n",
    "    \n",
    "    # save model corresponding to best epoch\n",
    "    checkpointer = ModelCheckpoint(filepath=path_to_data + 'model', \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True)\n",
    "    \n",
    "    # 200s/epoch on CPU - reaches 84.38% accuracy in 2 epochs\n",
    "    model.fit(my_docs_array_train, \n",
    "              my_labels_array_train,\n",
    "              batch_size = batch_size,\n",
    "              epochs = nb_epochs,\n",
    "              validation_data = (np.array(my_docs_array_test), my_labels_array_test), #specifying validation data as tuple\n",
    "              callbacks = [early_stopping,checkpointer])\n",
    "\n",
    "else:\n",
    "    model.load_weights(path_to_data + 'model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = extraction of attention coefficients = = = = =\n",
    "\n",
    "# define intermediate models (alternative to K.functions)\n",
    "\n",
    "\n",
    "# defining a Model named 'get_word_att_coeffs' that extracts the attention coefficients over the words in a sentence\n",
    "get_word_att_coeffs = Model(sent_ints, word_att_coeffs)\n",
    "\n",
    "# define a Model named 'get_sent_attention_coeffs' that extracts the attention coefficients over the sentences in a document\n",
    "# in each case, use the right inputs, and as outputs, the coefficients returned by the corresponding AttentionWithContext layer\n",
    "get_sent_attention_coeffs = Model(doc_ints, sent_att_coeffs)\n",
    "                            \n",
    "                            \n",
    "my_review = my_docs_array_test[-1:,:,:] # select last review\n",
    "# convert integer review to text\n",
    "index_to_word[1] = 'OOV'\n",
    "my_review_text = [[index_to_word[idx] for idx in sent if idx in index_to_word] for sent in my_review.tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29 There 's a sign on The Lost Highway that says : OOV SPOILERS OOV ( but you already knew that , did n't you ? )\n",
      "5.6 Since there 's a great deal of people that apparently did not get the point of this movie , I 'd like to contribute my interpretation of why the plot\n",
      "6.11 As others have pointed out , one single viewing of this movie is not sufficient .\n",
      "19.94 If you have the DVD of MD , you can OOV ' by looking at David Lynch 's 'Top 10 OOV to OOV MD ' ( but only upon second\n",
      "25.22 ; ) First of all , Mulholland Drive is downright brilliant .\n",
      "22.99 A masterpiece .\n",
      "15.85 This is the kind of movie that refuse to leave your head .\n",
      "('There', 0.05)\n",
      "(\"'s\", 0.2)\n",
      "('a', 0.24)\n",
      "('sign', 0.13)\n",
      "('on', 0.21)\n",
      "('The', 0.1)\n",
      "('Lost', 0.36)\n",
      "('Highway', 0.83)\n",
      "('that', 0.32)\n",
      "('says', 0.22)\n",
      "(':', 0.18)\n",
      "('OOV', 0.22)\n",
      "('SPOILERS', 0.26)\n",
      "('OOV', 0.21)\n",
      "('(', 0.16)\n",
      "('but', 0.06)\n",
      "('you', 0.05)\n",
      "('already', 0.05)\n",
      "('knew', 0.06)\n",
      "('that', 0.07)\n",
      "(',', 0.06)\n",
      "('did', 0.02)\n",
      "(\"n't\", 0.03)\n",
      "('you', 0.04)\n",
      "('?', 0.05)\n",
      "(')', 0.11)\n",
      "= = = =\n",
      "('Since', 0.15)\n",
      "('there', 0.12)\n",
      "(\"'s\", 0.23)\n",
      "('a', 0.26)\n",
      "('great', 0.55)\n",
      "('deal', 0.2)\n",
      "('of', 0.15)\n",
      "('people', 0.17)\n",
      "('that', 0.13)\n",
      "('apparently', 0.06)\n",
      "('did', 0.03)\n",
      "('not', 0.06)\n",
      "('get', 0.05)\n",
      "('the', 0.11)\n",
      "('point', 0.11)\n",
      "('of', 0.11)\n",
      "('this', 0.1)\n",
      "('movie', 0.12)\n",
      "(',', 0.31)\n",
      "('I', 0.12)\n",
      "(\"'d\", 0.16)\n",
      "('like', 0.24)\n",
      "('to', 0.38)\n",
      "('contribute', 0.5)\n",
      "('my', 0.11)\n",
      "('interpretation', 0.53)\n",
      "('of', 0.12)\n",
      "('why', 0.07)\n",
      "('the', 0.22)\n",
      "('plot', 0.12)\n",
      "= = = =\n",
      "('As', 0.02)\n",
      "('others', 0.01)\n",
      "('have', 0.02)\n",
      "('pointed', 0.04)\n",
      "('out', 0.03)\n",
      "(',', 0.04)\n",
      "('one', 0.03)\n",
      "('single', 0.06)\n",
      "('viewing', 0.22)\n",
      "('of', 0.08)\n",
      "('this', 0.05)\n",
      "('movie', 0.05)\n",
      "('is', 0.14)\n",
      "('not', 0.4)\n",
      "('sufficient', 4.64)\n",
      "('.', 0.26)\n",
      "= = = =\n",
      "('If', 0.37)\n",
      "('you', 0.17)\n",
      "('have', 0.11)\n",
      "('the', 0.22)\n",
      "('DVD', 0.21)\n",
      "('of', 0.31)\n",
      "('MD', 0.44)\n",
      "(',', 0.36)\n",
      "('you', 0.23)\n",
      "('can', 0.29)\n",
      "('OOV', 0.52)\n",
      "(\"'\", 0.57)\n",
      "('by', 0.52)\n",
      "('looking', 0.37)\n",
      "('at', 1.08)\n",
      "('David', 4.1)\n",
      "('Lynch', 2.18)\n",
      "(\"'s\", 1.01)\n",
      "(\"'Top\", 1.72)\n",
      "('10', 0.09)\n",
      "('OOV', 0.42)\n",
      "('to', 0.57)\n",
      "('OOV', 0.52)\n",
      "('MD', 0.83)\n",
      "(\"'\", 0.8)\n",
      "('(', 0.43)\n",
      "('but', 0.22)\n",
      "('only', 0.12)\n",
      "('upon', 0.59)\n",
      "('second', 0.56)\n",
      "= = = =\n",
      "(';', 2.18)\n",
      "(')', 1.59)\n",
      "('First', 0.39)\n",
      "('of', 1.02)\n",
      "('all', 1.5)\n",
      "(',', 1.4)\n",
      "('Mulholland', 2.44)\n",
      "('Drive', 0.44)\n",
      "('is', 0.81)\n",
      "('downright', 9.39)\n",
      "('brilliant', 2.88)\n",
      "('.', 1.15)\n",
      "= = = =\n",
      "('A', 6.37)\n",
      "('masterpiece', 7.96)\n",
      "('.', 8.52)\n",
      "= = = =\n",
      "('This', 0.22)\n",
      "('is', 0.81)\n",
      "('the', 2.78)\n",
      "('kind', 4.87)\n",
      "('of', 1.32)\n",
      "('movie', 0.45)\n",
      "('that', 0.87)\n",
      "('refuse', 0.7)\n",
      "('to', 0.76)\n",
      "('leave', 0.62)\n",
      "('your', 0.36)\n",
      "('head', 1.69)\n",
      "('.', 0.39)\n",
      "= = = =\n",
      "('Highway', 0.83)\n",
      "('Lost', 0.36)\n",
      "('that', 0.32)\n",
      "('SPOILERS', 0.26)\n",
      "('a', 0.24)\n",
      "('says', 0.22)\n",
      "('OOV', 0.22)\n",
      "('on', 0.21)\n",
      "('OOV', 0.21)\n",
      "(\"'s\", 0.2)\n",
      "(':', 0.18)\n",
      "('(', 0.16)\n",
      "('sign', 0.13)\n",
      "(')', 0.11)\n",
      "('The', 0.1)\n",
      "('that', 0.07)\n",
      "('but', 0.06)\n",
      "('knew', 0.06)\n",
      "(',', 0.06)\n",
      "('There', 0.05)\n",
      "('you', 0.05)\n",
      "('already', 0.05)\n",
      "('?', 0.05)\n",
      "('you', 0.04)\n",
      "(\"n't\", 0.03)\n",
      "('did', 0.02)\n",
      "= = = =\n",
      "('great', 0.55)\n",
      "('interpretation', 0.53)\n",
      "('contribute', 0.5)\n",
      "('to', 0.38)\n",
      "(',', 0.31)\n",
      "('a', 0.26)\n",
      "('like', 0.24)\n",
      "(\"'s\", 0.23)\n",
      "('the', 0.22)\n",
      "('deal', 0.2)\n",
      "('people', 0.17)\n",
      "(\"'d\", 0.16)\n",
      "('Since', 0.15)\n",
      "('of', 0.15)\n",
      "('that', 0.13)\n",
      "('there', 0.12)\n",
      "('movie', 0.12)\n",
      "('I', 0.12)\n",
      "('of', 0.12)\n",
      "('plot', 0.12)\n",
      "('the', 0.11)\n",
      "('point', 0.11)\n",
      "('of', 0.11)\n",
      "('my', 0.11)\n",
      "('this', 0.1)\n",
      "('why', 0.07)\n",
      "('apparently', 0.06)\n",
      "('not', 0.06)\n",
      "('get', 0.05)\n",
      "('did', 0.03)\n",
      "= = = =\n",
      "('sufficient', 4.64)\n",
      "('not', 0.4)\n",
      "('.', 0.26)\n",
      "('viewing', 0.22)\n",
      "('is', 0.14)\n",
      "('of', 0.08)\n",
      "('single', 0.06)\n",
      "('this', 0.05)\n",
      "('movie', 0.05)\n",
      "('pointed', 0.04)\n",
      "(',', 0.04)\n",
      "('out', 0.03)\n",
      "('one', 0.03)\n",
      "('As', 0.02)\n",
      "('have', 0.02)\n",
      "('others', 0.01)\n",
      "= = = =\n",
      "('David', 4.1)\n",
      "('Lynch', 2.18)\n",
      "(\"'Top\", 1.72)\n",
      "('at', 1.08)\n",
      "(\"'s\", 1.01)\n",
      "('MD', 0.83)\n",
      "(\"'\", 0.8)\n",
      "('upon', 0.59)\n",
      "(\"'\", 0.57)\n",
      "('to', 0.57)\n",
      "('second', 0.56)\n",
      "('OOV', 0.52)\n",
      "('by', 0.52)\n",
      "('OOV', 0.52)\n",
      "('MD', 0.44)\n",
      "('(', 0.43)\n",
      "('OOV', 0.42)\n",
      "('If', 0.37)\n",
      "('looking', 0.37)\n",
      "(',', 0.36)\n",
      "('of', 0.31)\n",
      "('can', 0.29)\n",
      "('you', 0.23)\n",
      "('the', 0.22)\n",
      "('but', 0.22)\n",
      "('DVD', 0.21)\n",
      "('you', 0.17)\n",
      "('only', 0.12)\n",
      "('have', 0.11)\n",
      "('10', 0.09)\n",
      "= = = =\n",
      "('downright', 9.39)\n",
      "('brilliant', 2.88)\n",
      "('Mulholland', 2.44)\n",
      "(';', 2.18)\n",
      "(')', 1.59)\n",
      "('all', 1.5)\n",
      "(',', 1.4)\n",
      "('.', 1.15)\n",
      "('of', 1.02)\n",
      "('is', 0.81)\n",
      "('Drive', 0.44)\n",
      "('First', 0.39)\n",
      "= = = =\n",
      "('.', 8.52)\n",
      "('masterpiece', 7.96)\n",
      "('A', 6.37)\n",
      "= = = =\n",
      "('kind', 4.87)\n",
      "('the', 2.78)\n",
      "('head', 1.69)\n",
      "('of', 1.32)\n",
      "('that', 0.87)\n",
      "('is', 0.81)\n",
      "('to', 0.76)\n",
      "('refuse', 0.7)\n",
      "('leave', 0.62)\n",
      "('movie', 0.45)\n",
      "('.', 0.39)\n",
      "('your', 0.36)\n",
      "('This', 0.22)\n",
      "= = = =\n"
     ]
    }
   ],
   "source": [
    "# = = = attention over sentences in the document\n",
    "\n",
    "sent_coeffs = get_sent_attention_coeffs.predict(my_review)\n",
    "sent_coeffs = sent_coeffs[0,:,:]\n",
    "\n",
    "for elt in zip(sent_coeffs[:,0].tolist(),[' '.join(elt) for elt in my_review_text]):\n",
    "    print(round(elt[0]*100,2),elt[1])\n",
    "\n",
    "# = = = attention over words in each sentence\n",
    "\n",
    "my_review_tensor = _to_tensor(my_review,dtype='float32') # a layer, unlike a model, requires a TensorFlow tensor as input\n",
    "\n",
    "# applying the 'get_word_att_coeffs' model over all the sentences in 'my_review_tensor', and store the results as 'word_coeffs'\n",
    "word_coeffs = TimeDistributed(get_word_att_coeffs)(my_review_tensor)\n",
    "\n",
    "\n",
    "word_coeffs = K.eval(word_coeffs) # shape = (1, 7, 30, 1): (batch size, nb of sents in doc, nb of words per sent, coeff)\n",
    "\n",
    "word_coeffs = word_coeffs[0,:,:,0] # shape = (7, 30) (coeff for each word in each sentence)\n",
    "\n",
    "word_coeffs = sent_coeffs * word_coeffs # re-weigh according to sentence importance\n",
    "\n",
    "word_coeffs = np.round((word_coeffs*100).astype(np.float64),2)\n",
    "\n",
    "word_coeffs_list = word_coeffs.tolist()\n",
    "\n",
    "# match text and coefficients\n",
    "text_word_coeffs = [list(zip(words,word_coeffs_list[idx][:len(words)])) for idx,words in enumerate(my_review_text)]\n",
    "\n",
    "for sent in text_word_coeffs:\n",
    "    [print(elt) for elt in sent]\n",
    "    print('= = = =')\n",
    "\n",
    "# sort words by importance within each sentence\n",
    "text_word_coeffs_sorted = [sorted(elt,key=operator.itemgetter(1),reverse=True) for elt in text_word_coeffs]\n",
    "\n",
    "for sent in text_word_coeffs_sorted:\n",
    "    [print(elt) for elt in sent]\n",
    "    print('= = = =')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
