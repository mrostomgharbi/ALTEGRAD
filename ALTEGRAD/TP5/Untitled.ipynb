{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    https://github.com/richliao/textClassifier/issues/13#issuecomment-377323318\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    initially taken from: https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    \n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    \n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    \n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, return_coefficients=False,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        \n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        \n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "        \n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "        \n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        \n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "        \n",
    "        a = K.exp(ait)\n",
    "        \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        \n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), a]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'EuclideanKeyedVectors' on <module 'gensim.models.keyedvectors' from '/home/rostom/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-34956984e222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# = = = = = loading pretrained word vectors = = = = =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mwvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'word_vectors.kv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# vocab does not contain the OOV token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordEmbeddingsKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastTextKeyedVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'compatible_hash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'EuclideanKeyedVectors' on <module 'gensim.models.keyedvectors' from '/home/rostom/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py'>"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.backend.tensorflow_backend import _to_tensor\n",
    "from keras.layers import Input, Embedding, Dropout, Bidirectional, GRU, TimeDistributed, Dense\n",
    "\n",
    "path_root = 'for_moodle/'\n",
    "path_to_data = path_root + 'data/'\n",
    "\n",
    "sys.path.insert(0, path_root)\n",
    "\n",
    "from AttentionWithContext import AttentionWithContext\n",
    "\n",
    "def bidir_gru(my_seq,n_units):\n",
    "    '''\n",
    "    just a convenient wrapper for bidirectional RNN with GRU units\n",
    "    '''\n",
    "    return Bidirectional(### fill the gap ### # add a default GRU layer (https://keras.io/layers/recurrent/). You need to specify only the 'units' and 'return_sequences' arguments\n",
    "                         GRU(units=n_units, activation='tanh', return_sequences=True),\n",
    "                         merge_mode='concat', weights=None)(my_seq)\n",
    " \n",
    "# = = = = = parameters = = = = =\n",
    "\n",
    "n_units = 50\n",
    "drop_rate = 0.5 \n",
    "mfw_idx = 2 # index of the most frequent words in the dictionary. \n",
    "            # 0 is for the special padding token\n",
    "            # 1 is for the special out-of-vocabulary token\n",
    "\n",
    "padding_idx = 0\n",
    "oov_idx = 1\n",
    "batch_size = 32\n",
    "nb_epochs = 6\n",
    "my_optimizer = 'adam'\n",
    "my_patience = 2 # for early stopping strategy\n",
    "\n",
    "\n",
    "# = = = = = data loading = = = = =\n",
    "\n",
    "my_docs_array_train = np.load(path_to_data + 'docs_train.npy')\n",
    "my_docs_array_test = np.load(path_to_data + 'docs_test.npy')\n",
    "\n",
    "my_labels_array_train = np.load(path_to_data + 'labels_train.npy')\n",
    "my_labels_array_test = np.load(path_to_data + 'labels_test.npy')\n",
    "\n",
    "# load dictionary of word indexes (sorted by decreasing frequency across the corpus)\n",
    "with open(path_to_data + 'word_to_index.json', 'r') as my_file:\n",
    "    word_to_index = json.load(my_file)\n",
    "\n",
    "# invert mapping\n",
    "index_to_word = dict((v,k) for k,v in word_to_index.items())\n",
    "\n",
    "# = = = = = loading pretrained word vectors = = = = =\n",
    "\n",
    "wvs = KeyedVectors.load(path_to_data + 'word_vectors.kv', mmap='r')\n",
    "assert len(wvs.wv.vocab) == len(word_to_index) + 1 # vocab does not contain the OOV token\n",
    "\n",
    "word_vecs = wvs.wv.syn0\n",
    "\n",
    "pad_vec = np.random.normal(size=word_vecs.shape[1])\n",
    "\n",
    "# add Gaussian vector on top of embedding matrix (padding vector)\n",
    "word_vecs = np.insert(word_vecs,0,pad_vec,0)\n",
    "\n",
    "print('embeddings created')\n",
    "\n",
    "# reduce dimension with PCA (to reduce the number of parameters of the model)\n",
    "my_pca = PCA(n_components=64)\n",
    "embeddings_pca = my_pca.fit_transform(word_vecs)\n",
    "\n",
    "print('embeddings compressed')\n",
    "\n",
    "# = = = = = defining architecture = = = = =\n",
    "\n",
    "# = = = sentence encoder\n",
    "\n",
    "sent_ints = Input(shape=(my_docs_array_train.shape[2],)) # vec of ints of variable size\n",
    "\n",
    "sent_wv = Embedding(input_dim=embeddings_pca.shape[0], # vocab size\n",
    "                    output_dim=embeddings_pca.shape[1], # dimensionality of embedding space\n",
    "                    weights=[embeddings_pca],\n",
    "                    input_length=my_docs_array_train.shape[2],\n",
    "                    trainable=True\n",
    "                    )(sent_ints)\n",
    "\n",
    "sent_wv_dr = Dropout(drop_rate)(sent_wv)\n",
    "\n",
    "### fill the gap (3 gaps) ###\n",
    "# use bidir_gru, AttentionWithContext with return_coefficients=True, and Dropout\n",
    "\n",
    "sent_wa = bidir_gru(sent_wv_dr , n_units)\n",
    "\n",
    "sent_att_vec , word_att_coeffs = AttentionWithContext(return_coefficients=True)(sent_wa)\n",
    "\n",
    "sent_att_vec_dr = Dropout(drop_rate)(sent_att_vec)\n",
    "\n",
    "sent_encoder = Model(sent_ints,sent_att_vec_dr)\n",
    "\n",
    "# = = = document encoder\n",
    "\n",
    "doc_ints = Input(shape=(my_docs_array_train.shape[1],my_docs_array_train.shape[2],))\n",
    "\n",
    "### fill the gap (4 gaps) ###\n",
    "# use TimeDistributed (https://keras.io/layers/wrappers/), bidir_gru, AttentionWithContext with return_coefficients=True, and Dropout\n",
    "                  \n",
    "sent_att_vec_dr = TimeDistributed(sent_encoder)(doc_ints)\n",
    "\n",
    "doc_sa = bidir_gru(sent_att_vec_dr , n_units)\n",
    "\n",
    "doc_att_vec , sent_att_coeffs = AttentionWithContext(return_coefficients=True)(doc_sa)\n",
    "\n",
    "doc_att_vec_dr = Dropout(drop_rate)(doc_att_vec)\n",
    "\n",
    "preds = Dense(units=1,\n",
    "              activation='sigmoid')(doc_att_vec_dr)\n",
    "\n",
    "model = Model(doc_ints,preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer = my_optimizer,\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print('model compiled')\n",
    "\n",
    "# = = = = = training = = = = =\n",
    "\n",
    "loading_pretrained = False\n",
    "\n",
    "if not loading_pretrained:\n",
    "    early_stopping = EarlyStopping(monitor='val_acc', # go through epochs as long as accuracy on validation set increases\n",
    "                                   patience=my_patience,\n",
    "                                   mode='max')\n",
    "    \n",
    "    # save model corresponding to best epoch\n",
    "    checkpointer = ModelCheckpoint(filepath=path_to_data + 'model', \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True)\n",
    "    \n",
    "    # 200s/epoch on CPU - reaches 84.38% accuracy in 2 epochs\n",
    "    model.fit(my_docs_array_train, \n",
    "              my_labels_array_train,\n",
    "              batch_size = batch_size,\n",
    "              epochs = nb_epochs,\n",
    "              validation_data = (np.array(my_docs_array_test), my_labels_array_test),### fill the gap ### specify validation data as tuple\n",
    "              callbacks = [early_stopping,checkpointer])\n",
    "\n",
    "else:\n",
    "    model.load_weights(path_to_data + 'model')\n",
    "\n",
    "\n",
    "# = = = = = extraction of attention coefficients = = = = =\n",
    "\n",
    "# define intermediate models (alternative to K.functions)\n",
    "\n",
    "### fill the gap (2 gaps) ###\n",
    "# define a Model named 'get_word_att_coeffs' that extracts the attention coefficients over the words in a sentence\n",
    "get_word_att_coeffs = Model(sent_ints, word_att_coeffs)\n",
    "\n",
    "# define a Model named 'get_sent_attention_coeffs' that extracts the attention coefficients over the sentences in a document\n",
    "# in each case, use the right inputs, and as outputs, the coefficients returned by the corresponding AttentionWithContext layer\n",
    "get_sent_attention_coeffs = Model(doc_ints, sent_att_coeffs)\n",
    "                            \n",
    "                            \n",
    "my_review = my_docs_array_test[-1:,:,:] # select last review\n",
    "# convert integer review to text\n",
    "index_to_word[1] = 'OOV'\n",
    "my_review_text = [[index_to_word[idx] for idx in sent if idx in index_to_word] for sent in my_review.tolist()[0]]\n",
    "\n",
    "# = = = attention over sentences in the document\n",
    "\n",
    "sent_coeffs = get_sent_attention_coeffs.predict(my_review)\n",
    "sent_coeffs = sent_coeffs[0,:,:]\n",
    "\n",
    "for elt in zip(sent_coeffs[:,0].tolist(),[' '.join(elt) for elt in my_review_text]):\n",
    "    print(round(elt[0]*100,2),elt[1])\n",
    "\n",
    "# = = = attention over words in each sentence\n",
    "\n",
    "my_review_tensor = _to_tensor(my_review,dtype='float32') # a layer, unlike a model, requires a TensorFlow tensor as input\n",
    "\n",
    "### fill the gap (one line) ###\n",
    "# apply the 'get_word_att_coeffs' model over all the sentences in 'my_review_tensor', and store the results as 'word_coeffs'\n",
    "word_coeffs = TimeDistributed(get_word_att_coeffs)(my_review_tensor)\n",
    "\n",
    "\n",
    "word_coeffs = K.eval(word_coeffs) # shape = (1, 7, 30, 1): (batch size, nb of sents in doc, nb of words per sent, coeff)\n",
    "\n",
    "word_coeffs = word_coeffs[0,:,:,0] # shape = (7, 30) (coeff for each word in each sentence)\n",
    "\n",
    "word_coeffs = sent_coeffs * word_coeffs # re-weigh according to sentence importance\n",
    "\n",
    "word_coeffs = np.round((word_coeffs*100).astype(np.float64),2)\n",
    "\n",
    "word_coeffs_list = word_coeffs.tolist()\n",
    "\n",
    "# match text and coefficients\n",
    "text_word_coeffs = [list(zip(words,word_coeffs_list[idx][:len(words)])) for idx,words in enumerate(my_review_text)]\n",
    "\n",
    "for sent in text_word_coeffs:\n",
    "    [print(elt) for elt in sent]\n",
    "    print('= = = =')\n",
    "\n",
    "# sort words by importance within each sentence\n",
    "text_word_coeffs_sorted = [sorted(elt,key=operator.itemgetter(1),reverse=True) for elt in text_word_coeffs]\n",
    "\n",
    "for sent in text_word_coeffs_sorted:\n",
    "    [print(elt) for elt in sent]\n",
    "    print('= = = =')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
